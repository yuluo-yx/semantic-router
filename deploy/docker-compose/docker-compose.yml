services:

  # Semantic Router External Processor Service
  semantic-router:
    image: ghcr.io/vllm-project/semantic-router/extproc:latest
    container_name: semantic-router
    ports:
      - "50051:50051"
    volumes:
      - ../../config:/app/config:ro,z
      - ../../models:/app/models:ro,z
      - ~/.cache/huggingface:/root/.cache/huggingface:z
    environment:
      - LD_LIBRARY_PATH=/app/lib
      # Use main config by default; override via CONFIG_FILE if needed
      - CONFIG_FILE=${CONFIG_FILE:-/app/config/config.yaml}
      # Optional informational envs (router reads YAML for tracing config)
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4317
      - OTEL_SERVICE_NAME=vllm-semantic-router
      - HUGGINGFACE_HUB_CACHE=/root/.cache/huggingface
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - HF_TOKEN=${HF_TOKEN:-}  # Optional: Required for downloading gated models (e.g., embeddinggemma-300m)
      - HUGGINGFACE_HUB_TOKEN=${HF_TOKEN:-}  # Alternative env var name used by huggingface-cli
    networks:
      - semantic-network
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      milvus:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 60
      start_period: 1800s

  # Envoy Proxy Service
  envoy:
    image: envoyproxy/envoy:v1.31.7
    container_name: envoy-proxy
    security_opt:
      - label=disable
    # File descriptor limits for handling many connections
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "8801:8801"  # Main proxy port
      - "19000:19000"  # Admin interface
    volumes:
      - ./addons/envoy.yaml:/etc/envoy/envoy.yaml:ro,z
    command: ["/usr/local/bin/envoy", "-c", "/etc/envoy/envoy.yaml", "--component-log-level", "ext_proc:trace,router:trace,http:trace"]
    depends_on:
      semantic-router:
        condition: service_healthy
    networks:
      - semantic-network
    healthcheck:
      test: ["CMD", "bash", "-c", "(echo -e 'GET /ready HTTP/1.1\\r\\nHost: localhost\\r\\n\\r\\n' >&3; timeout 2 cat <&3) 3<>/dev/tcp/localhost/19000 | grep -q LIVE"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Mock vLLM service for testing profile
  mock-vllm:
    build:
      context: ../../tools/mock-vllm
      dockerfile: Dockerfile
    container_name: mock-vllm
    profiles: ["testing"]
    ports:
      - "8000:8000"
    networks:
      semantic-network:
        ipv4_address: 172.28.0.10
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s

  # Jaeger for distributed tracing (OTLP gRPC + UI)
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jaeger
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "4318:4317"  # OTLP gRPC (mapped to 4318 on host to avoid conflicts)
      - "16686:16686"  # Web UI
    networks:
      - semantic-network

  # Prometheus and Grafana for observability
  prometheus:
    image: prom/prometheus:v2.53.0
    container_name: prometheus
    volumes:
      - ./addons/prometheus.yaml:/etc/prometheus/prometheus.yaml:ro,z
      - prometheus-data:/prometheus
    command:
      - --config.file=/etc/prometheus/prometheus.yaml
      - --storage.tsdb.retention.time=15d
    ports:
      - "9090:9090"
    networks:
      - semantic-network

  grafana:
    image: grafana/grafana:11.5.1
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - PROMETHEUS_URL=prometheus:9090
    ports:
      - "3000:3000"
    volumes:
      - ./addons/grafana.ini:/etc/grafana/grafana.ini:ro,z
      - ./addons/grafana-datasource.yaml:/etc/grafana/provisioning/datasources/datasource.yaml:ro,z
      - ./addons/grafana-datasource-jaeger.yaml:/etc/grafana/provisioning/datasources/datasource_jaeger.yaml:ro,z
      - ./addons/grafana-dashboard.yaml:/etc/grafana/provisioning/dashboards/dashboard.yaml:ro,z
      - ./addons/llm-router-dashboard.json:/etc/grafana/provisioning/dashboards/llm-router-dashboard.json:ro,z
      - grafana-data:/var/lib/grafana
    networks:
      - semantic-network
    depends_on:
      - prometheus

  # LLM Katan service for testing
  llm-katan:
    image: ${LLM_KATAN_IMAGE:-ghcr.io/vllm-project/semantic-router/llm-katan:latest}
    container_name: llm-katan
    profiles: ["testing", "llm-katan"]
    ports:
      - "8002:8002"
    environment:
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN:-}
      - HF_HUB_ENABLE_HF_TRANSFER=1
    volumes:
      - ../../models:/app/models:ro,z
      - hf-cache:/home/llmkatan/.cache/huggingface
    networks:
      semantic-network:
        ipv4_address: 172.28.0.20
    command: ["llm-katan", "--model", "/app/models/Qwen/Qwen3-0.6B", "--served-model-name", "qwen3", "--host", "0.0.0.0", "--port", "8002"]
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8002/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Semantic Router Dashboard
  dashboard:
    # Use pre-built image from GHCR, fallback to local build for development
    # image: ${DASHBOARD_IMAGE:-ghcr.io/vllm-project/semantic-router/dashboard:latest}
    build:
      context: ../../
      dockerfile: dashboard/backend/Dockerfile
    container_name: semantic-router-dashboard
    # Note: Entrypoint script handles config file permissions and switches to nonroot user
    environment:
      - DASHBOARD_PORT=8700
      - TARGET_GRAFANA_URL=http://grafana:3000
      - TARGET_PROMETHEUS_URL=http://prometheus:9090
      - TARGET_JAEGER_URL=http://jaeger:16686
      - TARGET_ROUTER_API_URL=http://semantic-router:8080
      - TARGET_ROUTER_METRICS_URL=http://semantic-router:9190/metrics
      - TARGET_ENVOY_URL=http://envoy:8801
      - ROUTER_CONFIG_PATH=/app/config/config.yaml
    volumes:
      - ../../config:/app/config:rw,z
    ports:
      - "8700:8700"
    networks:
      - semantic-network
    depends_on:
      semantic-router:
        condition: service_healthy
      grafana:
        condition: service_started
      prometheus:
        condition: service_started
      envoy:
        condition: service_started
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8700/healthz"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Redis for router replay storage
  redis:
    image: redis:7-alpine
    container_name: semantic-router-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    networks:
      - semantic-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # PostgreSQL for router replay storage
  postgres:
    image: postgres:16-alpine
    container_name: semantic-router-postgres
    environment:
      - POSTGRES_DB=semantic_router
      - POSTGRES_USER=router
      - POSTGRES_PASSWORD=router_password
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - semantic-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U router -d semantic_router"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Milvus Standalone for router replay storage
  etcd:
    image: quay.io/coreos/etcd:v3.5.5
    container_name: milvus-etcd
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - etcd-data:/etcd
    networks:
      - semantic-network
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 20s
      retries: 3

  minio:
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    container_name: milvus-minio
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - minio-data:/minio_data
    networks:
      - semantic-network
    command: minio server /minio_data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  milvus:
    image: milvusdb/milvus:v2.4.1
    container_name: semantic-router-milvus
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
    volumes:
      - milvus-data:/var/lib/milvus
    ports:
      - "19530:19530"
      - "9091:9091"
    networks:
      - semantic-network
    depends_on:
      - etcd
      - minio
    command: ["milvus", "run", "standalone"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
      interval: 30s
      timeout: 20s
      retries: 5
      start_period: 90s

networks:
  semantic-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

volumes:
  models-cache:
    driver: local
  prometheus-data:
  grafana-data:
  hf-cache:
  redis-data:
  postgres-data:
  etcd-data:
  minio-data:
  milvus-data:

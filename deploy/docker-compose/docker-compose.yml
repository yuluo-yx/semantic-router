services:

  # Semantic Router External Processor Service
  semantic-router:
    image: ghcr.io/vllm-project/semantic-router/extproc:latest
    container_name: semantic-router
    ports:
      - "50051:50051"
    volumes:
      - ../../config:/app/config:ro
      - ../../models:/app/models:ro
    environment:
      - LD_LIBRARY_PATH=/app/lib
      - CONFIG_FILE=${CONFIG_FILE:-/app/config/config.yaml}
    networks:
      - semantic-network
    healthcheck:
      test: ["CMD", "curl", "-f", "localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # Envoy Proxy Service
  envoy:
    image: envoyproxy/envoy:v1.31.7
    container_name: envoy-proxy
    ports:
      - "8801:8801"  # Main proxy port
      - "19000:19000"  # Admin interface
    volumes:
      - ./addons/envoy.yaml:/etc/envoy/envoy.yaml:ro
    command: ["/usr/local/bin/envoy", "-c", "/etc/envoy/envoy.yaml", "--component-log-level", "ext_proc:trace,router:trace,http:trace"]
    depends_on:
      semantic-router:
        condition: service_healthy
    networks:
      - semantic-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19000/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Mock vLLM service for testing profile
  mock-vllm:
    build:
      context: ../../tools/mock-vllm
      dockerfile: Dockerfile
    container_name: mock-vllm
    profiles: ["testing"]
    ports:
      - "8000:8000"
    networks:
      semantic-network:
        ipv4_address: 172.28.0.10
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s

  # Prometheus and Grafana for observability
  prometheus:
    image: prom/prometheus:v2.53.0
    container_name: prometheus
    volumes:
      - ./addons/prometheus.yaml:/etc/prometheus/prometheus.yaml:ro
      - prometheus-data:/prometheus
    command:
      - --config.file=/etc/prometheus/prometheus.yaml
      - --storage.tsdb.retention.time=15d
    ports:
      - "9090:9090"
    networks:
      - semantic-network

  grafana:
    image: grafana/grafana:11.5.1
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - PROMETHEUS_URL=prometheus:9090
    ports:
      - "3000:3000"
    volumes:
      - ./addons/grafana.ini:/etc/grafana/grafana.ini:ro
      - ./addons/grafana-datasource.yaml:/etc/grafana/provisioning/datasources/datasource.yaml:ro
      - ./addons/grafana-dashboard.yaml:/etc/grafana/provisioning/dashboards/dashboard.yaml:ro
      - ./addons/llm-router-dashboard.json:/etc/grafana/provisioning/dashboards/llm-router-dashboard.json:ro
      - grafana-data:/var/lib/grafana
    networks:
      - semantic-network
    depends_on:
      - prometheus

  # Open WebUI (optional, used by Dashboard Playground)
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    ports:
      - "3001:8080"
    environment:
      # Add any Open WebUI env here if needed (auth, providers, etc.)
      - WEBUI_NAME=Open WebUI
      # Route Open WebUI's OpenAI-compatible calls through Pipelines by default
      - OPENAI_API_BASE_URL=http://pipelines:9099
      - OPENAI_API_KEY=0p3n-w3bu!
    volumes:
      - openwebui-data:/app/backend/data
    networks:
      - semantic-network

  # Open WebUI Pipelines server (executes Python pipelines)
  pipelines:
    image: ghcr.io/open-webui/pipelines:main
    container_name: pipelines
    # Optional: expose on host for debugging
    # ports:
    #   - "9099:9099"
    environment:
      # Allow pipelines to reach services running on host if ever required
      - PYTHONUNBUFFERED=1
    volumes:
      # Persistent pipelines storage (auto-loaded on start)
      - openwebui-pipelines:/app/pipelines
      # Mount our vLLM Semantic Router pipeline
      - ./addons/vllm_semantic_router_pipe.py:/app/pipelines/vllm_semantic_router_pipe.py:ro
    networks:
      - semantic-network

  # LLM Katan service for testing
  llm-katan:
    build:
      context: ../../e2e-tests/llm-katan
      dockerfile: Dockerfile
    container_name: llm-katan
    profiles: ["testing", "llm-katan"]
    ports:
      - "8002:8000"
    environment:
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN:-}
    networks:
      - semantic-network
    command: ["llm-katan", "--model", "Qwen/Qwen3-0.6B", "--host", "0.0.0.0", "--port", "8000"]

  # Semantic Router Dashboard
  dashboard:
    # Use pre-built image from GHCR, fallback to local build for development
    image: ${DASHBOARD_IMAGE:-ghcr.io/vllm-project/semantic-router/dashboard:latest}
    build:
      context: ../../
      dockerfile: dashboard/backend/Dockerfile
    container_name: semantic-router-dashboard
    command: ["/app/dashboard-backend", "-port=8700", "-static=/app/frontend", "-config=/app/config/config.yaml"]
    environment:
      - DASHBOARD_PORT=8700
      - TARGET_GRAFANA_URL=http://grafana:3000
      - TARGET_PROMETHEUS_URL=http://prometheus:9090
      - TARGET_ROUTER_API_URL=http://semantic-router:8080
      - TARGET_ROUTER_METRICS_URL=http://semantic-router:9190/metrics
      - TARGET_OPENWEBUI_URL=http://openwebui:8080
      - ROUTER_CONFIG_PATH=/app/config/config.yaml
    volumes:
      - ../../config:/app/config:rw
    ports:
      - "8700:8700"
    networks:
      - semantic-network
    depends_on:
      semantic-router:
        condition: service_healthy
      grafana:
        condition: service_started
      prometheus:
        condition: service_started
      openwebui:
        condition: service_started
      pipelines:
        condition: service_started
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8700/healthz"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

networks:
  semantic-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

volumes:
  models-cache:
    driver: local
  prometheus-data:
  grafana-data:
  openwebui-data:
  openwebui-pipelines:

# Minimal Docker Compose for CI testing
# This file contains only essential services needed for integration testing.
# Excludes: grafana, prometheus, jaeger, dashboard
#
# Usage:
#   make docker-compose-up-ci
#   # or directly:
#   docker compose -f deploy/docker-compose/docker-compose.ci.yml up -d

services:

  # Semantic Router External Processor Service
  semantic-router:
    build:
      context: ../..
      dockerfile: tools/docker/Dockerfile.extproc
    image: ghcr.io/vllm-project/semantic-router/extproc:latest
    container_name: semantic-router
    ports:
      - "50051:50051"  # gRPC for ExtProc
      - "8080:8080"    # HTTP API (health, classify, metrics)
    volumes:
      - ../../config:/app/config:ro,z
      - ../../models:/app/models:z  # Writable for model downloads
      - ~/.cache/huggingface:/root/.cache/huggingface:z
    environment:
      - LD_LIBRARY_PATH=/app/lib
      - CONFIG_FILE=${CONFIG_FILE:-/app/config/config.yaml}
      - HUGGINGFACE_HUB_CACHE=/root/.cache/huggingface
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - HF_TOKEN=${HF_TOKEN:-}  # Optional: Required for downloading gated models (e.g., embeddinggemma-300m)
      - HUGGINGFACE_HUB_TOKEN=${HF_TOKEN:-}  # Alternative env var name used by huggingface-cli
    networks:
      - semantic-network
    healthcheck:
      test: ["CMD", "curl", "-f", "localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 60
      start_period: 600s

  # Envoy Proxy Service
  envoy:
    image: envoyproxy/envoy:v1.31.7
    container_name: envoy-proxy
    security_opt:
      - label=disable
    ports:
      - "8801:8801"  # Main proxy port
      - "19000:19000"  # Admin interface
    volumes:
      - ./addons/envoy.yaml:/etc/envoy/envoy.yaml:ro,z
    command: ["/usr/local/bin/envoy", "-c", "/etc/envoy/envoy.yaml", "--component-log-level", "ext_proc:debug,router:debug"]
    depends_on:
      semantic-router:
        condition: service_healthy
    networks:
      - semantic-network
    healthcheck:
      test: ["CMD", "bash", "-c", "(echo -e 'GET /ready HTTP/1.1\\r\\nHost: localhost\\r\\n\\r\\n' >&3; timeout 2 cat <&3) 3<>/dev/tcp/localhost/19000 | grep -q LIVE"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # LLM Katan service - lightweight mock LLM for testing
  llm-katan:
    image: ghcr.io/vllm-project/semantic-router/llm-katan:latest
    container_name: llm-katan
    ports:
      - "8002:8002"
    environment:
      - HF_HUB_ENABLE_HF_TRANSFER=1
    volumes:
      - ../../models:/app/models:z  # Writable for model downloads
      - hf-cache:/home/llmkatan/.cache/huggingface
    networks:
      semantic-network:
        ipv4_address: 172.28.0.20
    command: ["llm-katan", "--model", "/app/models/Qwen/Qwen3-0.6B", "--served-model-name", "qwen3", "--host", "0.0.0.0", "--port", "8002"]
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8002/health"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 120s

networks:
  semantic-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

volumes:
  hf-cache:

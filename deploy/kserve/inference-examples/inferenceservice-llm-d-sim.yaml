apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llm-d-sim
  labels:
    app: llm-d-sim
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    containers:
      - name: llm-d-inference-sim
        image: ghcr.io/llm-d/llm-d-inference-sim:v0.6.1
        imagePullPolicy: Always
        args:
          - --port
          - "8080"
          - --model
          - facebook/opt-125m
          - --mode
          - random
        ports:
          - containerPort: 8080
            protocol: TCP

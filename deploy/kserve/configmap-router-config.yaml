apiVersion: v1
kind: ConfigMap
metadata:
  name: semantic-router-kserve-config
  labels:
    app: semantic-router
    component: config
data:
  config.yaml: |
    bert_model:
      model_id: models/{{EMBEDDING_MODEL}}
      threshold: 0.6
      use_cpu: true

    semantic_cache:
      enabled: true
      backend_type: "memory"
      similarity_threshold: 0.8
      max_entries: 1000
      ttl_seconds: 3600
      eviction_policy: "fifo"
      use_hnsw: true
      hnsw_m: 16
      hnsw_ef_construction: 200
      embedding_model: "bert"

    tools:
      enabled: false  # Disabled - tools_db.json not included in KServe deployment
      top_k: 3
      similarity_threshold: 0.2
      tools_db_path: "config/tools_db.json"
      fallback_to_empty: true

    prompt_guard:
      enabled: true
      use_modernbert: true
      model_id: "models/mom-jailbreak-classifier"
      threshold: 0.7
      use_cpu: true
      jailbreak_mapping_path: "models/mom-jailbreak-classifier/jailbreak_type_mapping.json"

    # vLLM Endpoints Configuration - Using KServe InferenceService with Istio
    # IMPORTANT: Using stable ClusterIP service (not pod IP or headless service)
    # - KServe creates headless service by default (no stable ClusterIP)
    # - deploy.sh creates a stable ClusterIP service for consistent routing
    # - Service ClusterIP remains stable even when predictor pods restart
    # - Use HTTP (not HTTPS) on port 8080 - Istio handles mTLS
    # Template variables: {{INFERENCESERVICE_NAME}}, {{PREDICTOR_SERVICE_IP}}
    vllm_endpoints:
      - name: "{{INFERENCESERVICE_NAME}}-endpoint"
        address: "{{PREDICTOR_SERVICE_IP}}"  # Stable service ClusterIP (auto-populated by deploy script)
        port: 8080  # Container port (HTTP - Istio provides mTLS)
        weight: 1

    model_config:
      # KServe InferenceService model configuration
      # Template variable: {{MODEL_NAME}}, {{INFERENCESERVICE_NAME}}
      "{{MODEL_NAME}}":
        reasoning_family: "qwen3"  # Adjust based on model family: qwen3, deepseek, gpt, gpt-oss
        preferred_endpoints: ["{{INFERENCESERVICE_NAME}}-endpoint"]
          allow_by_default: true
          pii_types_allowed: ["EMAIL_ADDRESS"]

    # Classifier configuration
    classifier:
      category_model:
        model_id: "models/mom-domain-classifier"
        use_modernbert: true
        threshold: 0.6
        use_cpu: true
        category_mapping_path: "models/mom-domain-classifier/category_mapping.json"
      pii_model:
        model_id: "models/pii_classifier_modernbert-base_presidio_token_model"
        use_modernbert: true
        threshold: 0.7
        use_cpu: true
        pii_mapping_path: "models/mom-pii-classifier/pii_type_mapping.json"

    # Categories define domain metadata only (no routing logic)
    categories:
      - name: business
        description: "Business and management related queries"
        mmlu_categories: ["business"]
      - name: law
        description: "Legal questions and law-related topics"
        mmlu_categories: ["law"]
      - name: psychology
        description: "Psychology and mental health topics"
        mmlu_categories: ["psychology"]
      - name: biology
        description: "Biology and life sciences questions"
        mmlu_categories: ["biology"]
      - name: chemistry
        description: "Chemistry and chemical sciences questions"
        mmlu_categories: ["chemistry"]
      - name: history
        description: "Historical questions and cultural topics"
        mmlu_categories: ["history"]
      - name: other
        description: "General knowledge and miscellaneous topics"
        mmlu_categories: ["other"]
      - name: health
        description: "Health and medical information queries"
        mmlu_categories: ["health"]
      - name: economics
        description: "Economics and financial topics"
        mmlu_categories: ["economics"]
      - name: math
        description: "Mathematics and quantitative reasoning"
        mmlu_categories: ["math"]
      - name: physics
        description: "Physics and physical sciences"
        mmlu_categories: ["physics"]
      - name: computer_science
        description: "Computer science and programming"
        mmlu_categories: ["computer_science"]
      - name: philosophy
        description: "Philosophy and ethical questions"
        mmlu_categories: ["philosophy"]
      - name: engineering
        description: "Engineering and technical problem-solving"
        mmlu_categories: ["engineering"]

    strategy: "priority"

    decisions:
      - name: "business_decision"
        description: "Business and management related queries"
        priority: 100
        rules:
          operator: "AND"
          conditions:
            - type: "domain"
              name: "business"
        modelRefs:
          - model: "{{MODEL_NAME}}"
            use_reasoning: false
        plugins:
          - type: "system_prompt"
            configuration:
              system_prompt: "You are a senior business consultant and strategic advisor."
          - type: "pii"
            configuration:
              enabled: true
              pii_types_allowed: []

      - name: "math_decision"
        description: "Mathematics and quantitative reasoning"
        priority: 100
        rules:
          operator: "AND"
          conditions:
            - type: "domain"
              name: "math"
        modelRefs:
          - model: "granite32-8b"
            use_reasoning: true
        plugins:
          - type: "system_prompt"
            configuration:
              system_prompt: "You are a mathematics expert."
          - type: "pii"
            configuration:
              enabled: true
              pii_types_allowed: []

      - name: "general_decision"
        description: "General knowledge and miscellaneous topics"
        priority: 50
        rules:
          operator: "AND"
          conditions:
            - type: "domain"
              name: "other"
        modelRefs:
          - model: "{{MODEL_NAME}}"
            use_reasoning: false
        plugins:
          - type: "system_prompt"
            configuration:
              system_prompt: "You are a helpful assistant."
          - type: "semantic-cache"
            configuration:
              enabled: true
              similarity_threshold: 0.75
          - type: "pii"
            configuration:
              enabled: true
              pii_types_allowed: []

    default_model: {{MODEL_NAME}}

    # Reasoning family configurations
    reasoning_families:
      deepseek:
        type: "chat_template_kwargs"
        parameter: "thinking"
      qwen3:
        type: "chat_template_kwargs"
        parameter: "enable_thinking"
      gpt-oss:
        type: "reasoning_effort"
        parameter: "reasoning_effort"
      gpt:
        type: "reasoning_effort"
        parameter: "reasoning_effort"

    default_reasoning_effort: high

    # API Configuration
    api:
      batch_classification:
        max_batch_size: 100
        concurrency_threshold: 5
        max_concurrency: 8
        metrics:
          enabled: true
          detailed_goroutine_tracking: true
          high_resolution_timing: false
          sample_rate: 1.0
          duration_buckets: [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10, 30]
          size_buckets: [1, 2, 5, 10, 20, 50, 100, 200]

    # Embedding Models Configuration (Optional)
    # These are SEPARATE from the bert_model above and are used for the /v1/embeddings API endpoint.
    # The bert_model (configured above) is used for semantic caching and tools similarity.
    #
    # To enable the embeddings API with Qwen3/Gemma models:
    # 1. Uncomment the section below
    # 2. Update the deployment init container to download these models
    # 3. Note: These models are large (~600MB each) and not required for routing functionality
    #
    # embedding_models:
    #   qwen3_model_path: "models/mom-embedding-pro"
    #   gemma_model_path: "models/mom-embedding-flash"
    #   use_cpu: true

    # Observability Configuration
    observability:
      tracing:
        enabled: false
        provider: "opentelemetry"
        exporter:
          type: "stdout"
          endpoint: "localhost:4317"
          insecure: true
        sampling:
          type: "always_on"
          rate: 1.0
        resource:
          service_name: "vllm-semantic-router"
          service_version: "v0.1.0"
          deployment_environment: "production"

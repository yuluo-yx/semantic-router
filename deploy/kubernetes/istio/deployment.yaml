apiVersion: apps/v1
kind: Deployment
metadata:
  name: semantic-router
  namespace: vllm-semantic-router-system
  labels:
    app: semantic-router
spec:
  replicas: 1
  selector:
    matchLabels:
      app: semantic-router
  template:
    metadata:
      labels:
        app: semantic-router
    spec:
      initContainers:
        - name: model-downloader
          image: python:3.11-slim
          securityContext:
            runAsNonRoot: false
            allowPrivilegeEscalation: false
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -e
              # Check if all required models already exist in PVC; if yes, skip downloads entirely
              REQUIRED_DIRS=(
                "all-MiniLM-L12-v2"
                "category_classifier_modernbert-base_model"
                "pii_classifier_modernbert-base_model"
                "jailbreak_classifier_modernbert-base_model"
                "pii_classifier_modernbert-base_presidio_token_model"
                "Qwen3-Embedding-0.6B"
              )
              mkdir -p /app/models
              cd /app/models
              MISSING=false
              for d in "${REQUIRED_DIRS[@]}"; do
                if [ ! -d "$d" ]; then
                  MISSING=true
                  break
                fi
              done
              if [ "$MISSING" = false ]; then
                echo "All required models already present in PVC. Skipping download."
                exit 0
              fi

              echo "Installing Hugging Face CLI..."
              pip install --no-cache-dir huggingface_hub[cli]

              echo "Downloading missing models to persistent volume..."

              # Download all-MiniLM-L12-v2 model
              if [ ! -d "all-MiniLM-L12-v2" ]; then
                echo "Downloading all-MiniLM-L12-v2 model..."
                hf download sentence-transformers/all-MiniLM-L12-v2 --local-dir all-MiniLM-L12-v2
              else
                echo "all-MiniLM-L12-v2 model already exists, skipping..."
              fi

              # Download category classifier model
              if [ ! -d "category_classifier_modernbert-base_model" ]; then
                echo "Downloading category classifier model..."
                hf download LLM-Semantic-Router/category_classifier_modernbert-base_model --local-dir category_classifier_modernbert-base_model
              else
                echo "Category classifier model already exists, skipping..."
              fi

              # Download PII classifier model
              if [ ! -d "pii_classifier_modernbert-base_model" ]; then
                echo "Downloading PII classifier model..."
                hf download LLM-Semantic-Router/pii_classifier_modernbert-base_model --local-dir pii_classifier_modernbert-base_model
              else
                echo "PII classifier model already exists, skipping..."
              fi

              # Download jailbreak classifier model
              if [ ! -d "jailbreak_classifier_modernbert-base_model" ]; then
                echo "Downloading jailbreak classifier model..."
                hf download LLM-Semantic-Router/jailbreak_classifier_modernbert-base_model --local-dir jailbreak_classifier_modernbert-base_model
              else
                echo "Jailbreak classifier model already exists, skipping..."
              fi

              # Download PII token classifier model
              if [ ! -d "pii_classifier_modernbert-base_presidio_token_model" ]; then
                echo "Downloading PII token classifier model..."
                hf download LLM-Semantic-Router/pii_classifier_modernbert-base_presidio_token_model --local-dir pii_classifier_modernbert-base_presidio_token_model
              else
                echo "PII token classifier model already exists, skipping..."
              fi

              # Download Qwen3 Embedding model
              if [ ! -f "Qwen3-Embedding-0.6B/model.safetensors" ]; then
                echo "Downloading Qwen3-Embedding-0.6B model (missing model weights)..."
                rm -rf Qwen3-Embedding-0.6B
                hf download Qwen/Qwen3-Embedding-0.6B --local-dir Qwen3-Embedding-0.6B
                echo "Downloaded Qwen3-Embedding-0.6B files:"
                ls -la Qwen3-Embedding-0.6B/
              else
                echo "Qwen3-Embedding-0.6B model already exists with weights, skipping..."
              fi

              echo "All missing models downloaded successfully!"
              ls -la /app/models/
          env:
            - name: HF_HUB_CACHE
              value: /tmp/hf_cache
          # Increased resource requirements for init container to prevent timeouts during model download
          resources:
            requests:
              memory: "1Gi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "1"
          volumeMounts:
            - name: models-volume
              mountPath: /app/models
      containers:
        - name: semantic-router
          image: ghcr.io/vllm-project/semantic-router/extproc:latest
          args: ["--secure=true"]
          securityContext:
            runAsNonRoot: false
            allowPrivilegeEscalation: false
          ports:
            - containerPort: 50051
              name: grpc
              protocol: TCP
            - containerPort: 9190
              name: metrics
              protocol: TCP
            - containerPort: 8080
              name: classify-api
              protocol: TCP
          env:
            - name: LD_LIBRARY_PATH
              value: "/app/lib"
          volumeMounts:
            - name: config-volume
              mountPath: /app/config
              readOnly: true
            - name: models-volume
              mountPath: /app/models
          livenessProbe:
            tcpSocket:
              port: 50051
            initialDelaySeconds: 300  # Wait 5 minutes for model loading
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 5  # Allow more failures
          readinessProbe:
            tcpSocket:
              port: 50051
            initialDelaySeconds: 300  # Wait 5 minutes for model loading
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 5  # Allow more failures
          # Increased memory for multiple models including Qwen3-Embedding-0.6B
          resources:
            requests:
              memory: "4Gi"  # Increased to handle all models
              cpu: "1"
            limits:
              memory: "8Gi"  # Increased to prevent OOMKill
              cpu: "2"
      volumes:
        - name: config-volume
          configMap:
            name: semantic-router-config
        - name: models-volume
          persistentVolumeClaim:
            claimName: semantic-router-models

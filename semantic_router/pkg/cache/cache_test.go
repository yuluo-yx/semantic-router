package cache_test

import (
	"encoding/json"
	"fmt"
	"sync"
	"testing"
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"

	candle "github.com/redhat-et/semantic_route/candle-binding"
	"github.com/redhat-et/semantic_route/semantic_router/pkg/cache"
)

func TestCache(t *testing.T) {
	RegisterFailHandler(Fail)
	RunSpecs(t, "Cache Suite")
}

var _ = BeforeSuite(func() {
	err := candle.InitModel("", true)
	Expect(err).NotTo(HaveOccurred())
})

var _ = Describe("Cache Package", func() {
	var (
		semanticCache *cache.SemanticCache
		defaultOptions cache.SemanticCacheOptions
	)

	BeforeEach(func() {
		defaultOptions = cache.SemanticCacheOptions{
			SimilarityThreshold: 0.8,
			MaxEntries:          100,
			TTLSeconds:          3600,
			Enabled:             true,
		}
		semanticCache = cache.NewSemanticCache(defaultOptions)
	})

	Describe("NewSemanticCache", func() {
		It("should create a cache with correct options", func() {
			options := cache.SemanticCacheOptions{
				SimilarityThreshold: 0.9,
				MaxEntries:          50,
				TTLSeconds:          1800,
				Enabled:             true,
			}
			c := cache.NewSemanticCache(options)
			Expect(c).NotTo(BeNil())
			Expect(c.IsEnabled()).To(BeTrue())
		})

		It("should create a disabled cache when specified", func() {
			options := cache.SemanticCacheOptions{
				Enabled: false,
			}
			c := cache.NewSemanticCache(options)
			Expect(c.IsEnabled()).To(BeFalse())
		})
	})

	Describe("IsEnabled", func() {
		It("should return the correct enabled status", func() {
			enabledCache := cache.NewSemanticCache(cache.SemanticCacheOptions{Enabled: true})
			Expect(enabledCache.IsEnabled()).To(BeTrue())

			disabledCache := cache.NewSemanticCache(cache.SemanticCacheOptions{Enabled: false})
			Expect(disabledCache.IsEnabled()).To(BeFalse())
		})
	})

	Describe("AddEntry", func() {
		Context("when cache is enabled", func() {
			It("should add a complete entry successfully", func() {
				model := "gpt-4"
				query := "What is the capital of France?"
				requestBody := []byte(`{"model": "gpt-4", "messages": [{"role": "user", "content": "What is the capital of France?"}]}`)
				responseBody := []byte(`{"choices": [{"message": {"content": "Paris"}}]}`)

				err := semanticCache.AddEntry(model, query, requestBody, responseBody)
				Expect(err).NotTo(HaveOccurred())
			})

			It("should handle empty query gracefully", func() {
				model := "gpt-4"
				query := ""
				requestBody := []byte(`{"model": "gpt-4"}`)
				responseBody := []byte(`{"choices": []}`)

				err := semanticCache.AddEntry(model, query, requestBody, responseBody)
				// Should not error, but may not generate embedding for empty query
				// The actual behavior depends on the candle_binding implementation
				Expect(err).To(Or(BeNil(), HaveOccurred()))
			})

			Context("with max entries limit", func() {
				BeforeEach(func() {
					options := cache.SemanticCacheOptions{
						SimilarityThreshold: 0.8,
						MaxEntries:          3,
						TTLSeconds:          0, // No TTL for this test
						Enabled:             true,
					}
					semanticCache = cache.NewSemanticCache(options)
				})

				It("should enforce max entries limit by removing oldest entries", func() {
					// Add entries beyond the limit
					for i := 0; i < 5; i++ {
						query := fmt.Sprintf("Query %d", i)
						model := "gpt-4"
						requestBody := []byte(fmt.Sprintf(`{"query": "%s"}`, query))
						responseBody := []byte(fmt.Sprintf(`{"response": "Response %d"}`, i))

						err := semanticCache.AddEntry(model, query, requestBody, responseBody)
						Expect(err).To(Or(BeNil(), HaveOccurred())) // Embedding generation might fail in test
						
						// Small delay to ensure different timestamps
						time.Sleep(time.Millisecond)
					}

					// The cache should not exceed max entries
					// We can't directly access the entries count, but we can test the behavior
					// by checking that older entries are removed
				})
			})
		})

		Context("when cache is disabled", func() {
			BeforeEach(func() {
				options := cache.SemanticCacheOptions{Enabled: false}
				semanticCache = cache.NewSemanticCache(options)
			})

			It("should return immediately without error", func() {
				model := "gpt-4"
				query := "Test query"
				requestBody := []byte(`{"test": "data"}`)
				responseBody := []byte(`{"result": "success"}`)

				err := semanticCache.AddEntry(model, query, requestBody, responseBody)
				Expect(err).NotTo(HaveOccurred())
			})
		})
	})

	Describe("AddPendingRequest", func() {
		Context("when cache is enabled", func() {
			It("should add a pending request and return the query", func() {
				model := "gpt-4"
				query := "What is machine learning?"
				requestBody := []byte(`{"model": "gpt-4", "messages": [{"role": "user", "content": "What is machine learning?"}]}`)

				returnedQuery, err := semanticCache.AddPendingRequest(model, query, requestBody)
				Expect(err).To(Or(BeNil(), HaveOccurred())) // Embedding generation might fail
				if err == nil {
					Expect(returnedQuery).To(Equal(query))
				}
			})

			It("should handle empty query", func() {
				model := "gpt-4"
				query := ""
				requestBody := []byte(`{"model": "gpt-4"}`)

				returnedQuery, err := semanticCache.AddPendingRequest(model, query, requestBody)
				// Should handle empty query gracefully
				Expect(err).To(Or(BeNil(), HaveOccurred()))
				if err == nil {
					Expect(returnedQuery).To(Equal(query))
				}
			})
		})

		Context("when cache is disabled", func() {
			BeforeEach(func() {
				options := cache.SemanticCacheOptions{Enabled: false}
				semanticCache = cache.NewSemanticCache(options)
			})

			It("should return the query without processing", func() {
				model := "gpt-4"
				query := "Test query"
				requestBody := []byte(`{"test": "data"}`)

				returnedQuery, err := semanticCache.AddPendingRequest(model, query, requestBody)
				Expect(err).NotTo(HaveOccurred())
				Expect(returnedQuery).To(Equal(query))
			})
		})
	})

	Describe("UpdateWithResponse", func() {
		Context("when cache is enabled", func() {
			It("should update a pending request with response", func() {
				model := "gpt-4"
				query := "Test query for update"
				requestBody := []byte(`{"model": "gpt-4"}`)
				responseBody := []byte(`{"response": "test response"}`)

				// First add a pending request
				_, err := semanticCache.AddPendingRequest(model, query, requestBody)
				Expect(err).NotTo(HaveOccurred())

				// Then update it with response
				err = semanticCache.UpdateWithResponse(query, responseBody)
				Expect(err).NotTo(HaveOccurred())
			})

			It("should return error for non-existent pending request", func() {
				query := "Non-existent query"
				responseBody := []byte(`{"response": "test"}`)

				err := semanticCache.UpdateWithResponse(query, responseBody)
				Expect(err).To(HaveOccurred())
				Expect(err.Error()).To(ContainSubstring("no pending request found"))
			})
		})

		Context("when cache is disabled", func() {
			BeforeEach(func() {
				options := cache.SemanticCacheOptions{Enabled: false}
				semanticCache = cache.NewSemanticCache(options)
			})

			It("should return immediately without error", func() {
				query := "Test query"
				responseBody := []byte(`{"response": "test"}`)

				err := semanticCache.UpdateWithResponse(query, responseBody)
				Expect(err).NotTo(HaveOccurred())
			})
		})
	})

	Describe("FindSimilar", func() {
		Context("when cache is enabled", func() {
			It("should return cache miss for empty cache", func() {
				model := "gpt-4"
				query := "What is AI?"

				response, found, err := semanticCache.FindSimilar(model, query)
				Expect(err).NotTo(HaveOccurred())
				Expect(found).To(BeFalse())
				Expect(response).To(BeNil())
			})

			It("should handle empty query gracefully", func() {
				model := "gpt-4"
				query := ""

				response, found, err := semanticCache.FindSimilar(model, query)
				// Should handle empty query
				Expect(err).To(Or(BeNil(), HaveOccurred()))
				if err == nil {
					Expect(found).To(BeFalse())
					Expect(response).To(BeNil())
				}
			})

			Context("with entries in cache", func() {
				BeforeEach(func() {
					// Add some test entries if possible
					model := "gpt-4"
					query := "What is the weather?"
					requestBody := []byte(`{"model": "gpt-4"}`)
					responseBody := []byte(`{"weather": "sunny"}`)

					err := semanticCache.AddEntry(model, query, requestBody, responseBody)
					if err != nil {
						Skip("Skipping test due to candle_binding dependency")
					}
				})

				It("should find similar entries based on model matching", func() {
					model := "gpt-4"
					query := "Weather information"

									_, _, err := semanticCache.FindSimilar(model, query)
				Expect(err).NotTo(HaveOccurred())
				// Result depends on embedding similarity and threshold
				})

				It("should not find entries for different models", func() {
					model := "gpt-3.5-turbo" // Different model
					query := "What is the weather?"

					response, found, err := semanticCache.FindSimilar(model, query)
					Expect(err).NotTo(HaveOccurred())
					Expect(found).To(BeFalse())
					Expect(response).To(BeNil())
				})
			})
		})

		Context("when cache is disabled", func() {
			BeforeEach(func() {
				options := cache.SemanticCacheOptions{Enabled: false}
				semanticCache = cache.NewSemanticCache(options)
			})

			It("should return cache miss immediately", func() {
				model := "gpt-4"
				query := "Any query"

				response, found, err := semanticCache.FindSimilar(model, query)
				Expect(err).NotTo(HaveOccurred())
				Expect(found).To(BeFalse())
				Expect(response).To(BeNil())
			})
		})
	})

	Describe("TTL Functionality", func() {
		Context("with TTL enabled", func() {
			BeforeEach(func() {
				options := cache.SemanticCacheOptions{
					SimilarityThreshold: 0.8,
					MaxEntries:          100,
					TTLSeconds:          1, // 1 second TTL for testing
					Enabled:             true,
				}
				semanticCache = cache.NewSemanticCache(options)
			})

			It("should expire entries after TTL", func() {
				model := "gpt-4"
				query := "TTL test query"
				requestBody := []byte(`{"model": "gpt-4"}`)
				responseBody := []byte(`{"response": "test"}`)

				// Add entry
				err := semanticCache.AddEntry(model, query, requestBody, responseBody)
				if err != nil {
					Skip("Skipping test due to candle_binding dependency")
				}

				// Wait for TTL to expire
				time.Sleep(2 * time.Second)

				// Try to find the entry - should trigger cleanup and not find expired entry
				_, _, err = semanticCache.FindSimilar(model, query)
				Expect(err).NotTo(HaveOccurred())
				// Entry should be expired and not found, or found but will be cleaned up
			})
		})

		Context("without TTL", func() {
			BeforeEach(func() {
				options := cache.SemanticCacheOptions{
					SimilarityThreshold: 0.8,
					MaxEntries:          100,
					TTLSeconds:          0, // No TTL
					Enabled:             true,
				}
				semanticCache = cache.NewSemanticCache(options)
			})

			It("should not expire entries", func() {
				model := "gpt-4"
				query := "No TTL test query"
				requestBody := []byte(`{"model": "gpt-4"}`)
				responseBody := []byte(`{"response": "test"}`)

				// Add entry
				err := semanticCache.AddEntry(model, query, requestBody, responseBody)
				if err != nil {
					Skip("Skipping test due to candle_binding dependency")
				}

				// Wait some time
				time.Sleep(100 * time.Millisecond)

				// Entry should still be searchable
				_, _, err = semanticCache.FindSimilar(model, query)
				Expect(err).NotTo(HaveOccurred())
				// Without TTL, entry should persist (subject to similarity matching)
			})
		})
	})

	Describe("Concurrent Access", func() {
		It("should handle concurrent AddEntry calls safely", func() {
			const numGoroutines = 10
			var wg sync.WaitGroup
			errors := make([]error, numGoroutines)

			wg.Add(numGoroutines)
			for i := 0; i < numGoroutines; i++ {
				go func(index int) {
					defer wg.Done()
					model := "gpt-4"
					query := fmt.Sprintf("Concurrent query %d", index)
					requestBody := []byte(fmt.Sprintf(`{"index": %d}`, index))
					responseBody := []byte(fmt.Sprintf(`{"result": %d}`, index))

					err := semanticCache.AddEntry(model, query, requestBody, responseBody)
					errors[index] = err
				}(i)
			}

			wg.Wait()

			// Check that no race conditions occurred
			// Some errors might occur due to candle_binding, but no panics should happen
			for i := 0; i < numGoroutines; i++ {
				// We don't assert on specific errors since candle_binding might not be available
				// The important thing is that no race conditions or panics occurred
			}
		})

		It("should handle concurrent FindSimilar calls safely", func() {
			const numGoroutines = 10
			var wg sync.WaitGroup
			results := make([]bool, numGoroutines)
			errors := make([]error, numGoroutines)

			wg.Add(numGoroutines)
			for i := 0; i < numGoroutines; i++ {
				go func(index int) {
					defer wg.Done()
					model := "gpt-4"
					query := fmt.Sprintf("Search query %d", index)

					_, found, err := semanticCache.FindSimilar(model, query)
					results[index] = found
					errors[index] = err
				}(i)
			}

			wg.Wait()

			// Check that no race conditions occurred
			for i := 0; i < numGoroutines; i++ {
				// We don't assert on specific results since cache is likely empty
				// The important thing is that no race conditions or panics occurred
			}
		})

		It("should handle mixed concurrent operations safely", func() {
			const numGoroutines = 20
			var wg sync.WaitGroup

			wg.Add(numGoroutines)
			for i := 0; i < numGoroutines; i++ {
				go func(index int) {
					defer wg.Done()
					model := "gpt-4"
					query := fmt.Sprintf("Mixed operation query %d", index)

					if index%2 == 0 {
						// Add entry
						requestBody := []byte(fmt.Sprintf(`{"index": %d}`, index))
						responseBody := []byte(fmt.Sprintf(`{"result": %d}`, index))
						semanticCache.AddEntry(model, query, requestBody, responseBody)
					} else {
						// Search for similar
						semanticCache.FindSimilar(model, query)
					}
				}(i)
			}

			wg.Wait()
			// If we reach here without panic, the concurrent access handling is working
		})
	})

	Describe("ExtractQueryFromOpenAIRequest", func() {
		It("should extract model and query from valid OpenAI request", func() {
			request := cache.OpenAIRequest{
				Model: "gpt-4",
				Messages: []cache.ChatMessage{
					{Role: "system", Content: "You are a helpful assistant."},
					{Role: "user", Content: "What is the capital of France?"},
					{Role: "assistant", Content: "The capital of France is Paris."},
					{Role: "user", Content: "What about Germany?"},
				},
			}

			requestBody, err := json.Marshal(request)
			Expect(err).NotTo(HaveOccurred())

			model, query, err := cache.ExtractQueryFromOpenAIRequest(requestBody)
			Expect(err).NotTo(HaveOccurred())
			Expect(model).To(Equal("gpt-4"))
			Expect(query).To(Equal("What about Germany?")) // Should get the last user message
		})

		It("should handle request with only system messages", func() {
			request := cache.OpenAIRequest{
				Model: "gpt-3.5-turbo",
				Messages: []cache.ChatMessage{
					{Role: "system", Content: "You are a helpful assistant."},
				},
			}

			requestBody, err := json.Marshal(request)
			Expect(err).NotTo(HaveOccurred())

			model, query, err := cache.ExtractQueryFromOpenAIRequest(requestBody)
			Expect(err).NotTo(HaveOccurred())
			Expect(model).To(Equal("gpt-3.5-turbo"))
			Expect(query).To(BeEmpty()) // No user messages
		})

		It("should handle request with multiple user messages", func() {
			request := cache.OpenAIRequest{
				Model: "gpt-4",
				Messages: []cache.ChatMessage{
					{Role: "user", Content: "First user message"},
					{Role: "assistant", Content: "Assistant response"},
					{Role: "user", Content: "Second user message"},
					{Role: "user", Content: "Third user message"},
				},
			}

			requestBody, err := json.Marshal(request)
			Expect(err).NotTo(HaveOccurred())

			model, query, err := cache.ExtractQueryFromOpenAIRequest(requestBody)
			Expect(err).NotTo(HaveOccurred())
			Expect(model).To(Equal("gpt-4"))
			Expect(query).To(Equal("Third user message")) // Should get the last user message
		})

		It("should handle empty messages array", func() {
			request := cache.OpenAIRequest{
				Model:    "gpt-4",
				Messages: []cache.ChatMessage{},
			}

			requestBody, err := json.Marshal(request)
			Expect(err).NotTo(HaveOccurred())

			model, query, err := cache.ExtractQueryFromOpenAIRequest(requestBody)
			Expect(err).NotTo(HaveOccurred())
			Expect(model).To(Equal("gpt-4"))
			Expect(query).To(BeEmpty())
		})

		It("should return error for invalid JSON", func() {
			invalidJSON := []byte(`{"model": "gpt-4", "messages": [invalid json}`)

			model, query, err := cache.ExtractQueryFromOpenAIRequest(invalidJSON)
			Expect(err).To(HaveOccurred())
			Expect(model).To(BeEmpty())
			Expect(query).To(BeEmpty())
			Expect(err.Error()).To(ContainSubstring("invalid request body"))
		})

		It("should handle missing model field", func() {
			request := map[string]interface{}{
				"messages": []cache.ChatMessage{
					{Role: "user", Content: "Test message"},
				},
			}

			requestBody, err := json.Marshal(request)
			Expect(err).NotTo(HaveOccurred())

			model, query, err := cache.ExtractQueryFromOpenAIRequest(requestBody)
			Expect(err).NotTo(HaveOccurred())
			Expect(model).To(BeEmpty()) // Missing model field
			Expect(query).To(Equal("Test message"))
		})

		It("should handle request with empty content", func() {
			request := cache.OpenAIRequest{
				Model: "gpt-4",
				Messages: []cache.ChatMessage{
					{Role: "user", Content: ""},
					{Role: "user", Content: "Non-empty message"},
				},
			}

			requestBody, err := json.Marshal(request)
			Expect(err).NotTo(HaveOccurred())

			model, query, err := cache.ExtractQueryFromOpenAIRequest(requestBody)
			Expect(err).NotTo(HaveOccurred())
			Expect(model).To(Equal("gpt-4"))
			Expect(query).To(Equal("Non-empty message")) // Should get the last non-empty user message
		})
	})

	Describe("Edge Cases and Error Conditions", func() {
		It("should handle very large request/response bodies", func() {
			model := "gpt-4"
			query := "Large data test"
			largeData := make([]byte, 1024*1024) // 1MB of data
			for i := range largeData {
				largeData[i] = byte(i % 256)
			}

			err := semanticCache.AddEntry(model, query, largeData, largeData)
			// Should handle large data gracefully
			Expect(err).To(Or(BeNil(), HaveOccurred()))
		})

		It("should handle special characters in queries", func() {
			model := "gpt-4"
			query := "Query with special chars: ä½ å¥½, Ã©moji ðŸš€, and unicode âˆ€âˆƒâˆ…"
			requestBody := []byte(`{"special": "chars"}`)
			responseBody := []byte(`{"response": "special"}`)

			err := semanticCache.AddEntry(model, query, requestBody, responseBody)
			Expect(err).To(Or(BeNil(), HaveOccurred()))
		})

		It("should handle very long queries", func() {
			model := "gpt-4"
			query := string(make([]byte, 10000)) // Very long query
			for i := range query {
				query = query[:i] + "a"
			}
			requestBody := []byte(`{"long": "query"}`)
			responseBody := []byte(`{"response": "long"}`)

			err := semanticCache.AddEntry(model, query, requestBody, responseBody)
			Expect(err).To(Or(BeNil(), HaveOccurred()))
		})

		It("should handle nil request/response bodies", func() {
			model := "gpt-4"
			query := "Nil test"

			err := semanticCache.AddEntry(model, query, nil, nil)
			Expect(err).To(Or(BeNil(), HaveOccurred()))
		})
	})

	Describe("Similarity Threshold Edge Cases", func() {
		Context("with very low threshold", func() {
			BeforeEach(func() {
				options := cache.SemanticCacheOptions{
					SimilarityThreshold: 0.0, // Very low threshold
					MaxEntries:          100,
					TTLSeconds:          0,
					Enabled:             true,
				}
				semanticCache = cache.NewSemanticCache(options)
			})

			It("should potentially match more entries", func() {
				// Add an entry
				model := "gpt-4"
				query1 := "What is AI?"
				requestBody := []byte(`{"model": "gpt-4"}`)
				responseBody := []byte(`{"response": "AI info"}`)

				err := semanticCache.AddEntry(model, query1, requestBody, responseBody)
				if err != nil {
					Skip("Skipping test due to candle_binding dependency")
				}

				// Search with different query
				query2 := "Completely different query"
				_, _, err = semanticCache.FindSimilar(model, query2)
				Expect(err).NotTo(HaveOccurred())
				// With very low threshold, might find matches
			})
		})

		Context("with very high threshold", func() {
			BeforeEach(func() {
				options := cache.SemanticCacheOptions{
					SimilarityThreshold: 0.999, // Very high threshold
					MaxEntries:          100,
					TTLSeconds:          0,
					Enabled:             true,
				}
				semanticCache = cache.NewSemanticCache(options)
			})

			It("should rarely match entries", func() {
				// Add an entry
				model := "gpt-4"
				query1 := "What is AI?"
				requestBody := []byte(`{"model": "gpt-4"}`)
				responseBody := []byte(`{"response": "AI info"}`)

				err := semanticCache.AddEntry(model, query1, requestBody, responseBody)
				if err != nil {
					Skip("Skipping test due to candle_binding dependency")
				}

				// Search with slightly different query
				query2 := "What is artificial intelligence?"
				_, found, err := semanticCache.FindSimilar(model, query2)
				Expect(err).NotTo(HaveOccurred())
				// With very high threshold, should rarely find matches
				Expect(found).To(BeFalse())
			})
		})
	})
})
# Example Configuration for MCP-Based Category Classifier (HTTP Transport)
#
# This configuration demonstrates how to use an external MCP (Model Context Protocol)
# service via HTTP for category classification instead of the built-in Candle/ModernBERT models.
#
# Use cases:
# - Offload classification to a remote HTTP service
# - Use custom classification models not supported in-tree
# - Scale classification independently from the router
# - Integrate with existing ML infrastructure via REST API
#
# Note: This example uses HTTP transport. The MCP server should expose an HTTP endpoint
# that implements the MCP protocol (e.g., http://localhost:8080/mcp)

# BERT model for semantic caching and tool selection
bert_model:
  model_id: "sentence-transformers/all-MiniLM-L6-v2"
  threshold: 0.85
  use_cpu: true

# Classifier configuration
classifier:
  # Disable in-tree category classifier (leave model_id empty)
  category_model:
    model_id: ""  # Empty = disabled

  # Enable MCP-based category classifier (HTTP transport only)
  mcp_category_model:
    enabled: true                    # Enable MCP classifier
    transport_type: "http"           # HTTP transport
    url: "http://localhost:8090/mcp" # MCP server endpoint
    
    # tool_name: Optional - auto-discovers classification tool if not specified
    # Will search for tools like: classify_text, classify, categorize, etc.
    # Uncomment to explicitly specify:
    # tool_name: "classify_text"
    
    threshold: 0.6                   # Confidence threshold
    timeout_seconds: 30              # Request timeout

# Categories for routing queries
# 
# Categories are automatically loaded from MCP server via 'list_categories' tool.
# The MCP server controls BOTH classification AND routing decisions.
#
# How it works:
#   1. Router connects to MCP server at startup
#   2. Calls 'list_categories' tool and MCP returns:
#      {
#        "categories": ["math", "science", "technology", "history", "general"],
#        "category_system_prompts": {
#          "math": "You are a mathematics expert. When answering math questions...",
#          "science": "You are a science expert. When answering science questions...",
#          "technology": "You are a technology expert..."
#        },
#        "category_descriptions": {
#          "math": "Mathematical and computational queries",
#          "science": "Scientific concepts and queries"
#        }
#      }
#   3. For each request, calls 'classify_text' tool which returns:
#      {
#        "class": 3,
#        "confidence": 0.85,
#        "model": "openai/gpt-oss-20b",        # MCP decides which model to use
#        "use_reasoning": true                  # MCP decides whether to use reasoning
#      }
#   4. Router uses the model and reasoning settings from MCP response
#
# PER-CATEGORY SYSTEM PROMPT INJECTION:
#   - The MCP server provides SEPARATE system prompts for EACH category
#   - Each category gets its own specialized instructions and context
#   - The router stores these prompts and injects the appropriate one per query
#   - Use classifier.GetCategorySystemPrompt(categoryName) to retrieve for a specific category
#   - Examples:
#       * Math category: "You are a mathematics expert. Show step-by-step solutions..."
#       * Science category: "You are a science expert. Provide evidence-based answers..."
#       * Technology category: "You are a tech expert. Include practical code examples..."
#   - This allows domain-specific expertise per category
#
# BENEFITS:
#   - MCP server makes intelligent routing decisions per query
#   - No hardcoded routing rules needed in config
#   - MCP can adapt routing based on query complexity, content, etc.
#   - Centralized routing logic and per-category system prompts in MCP server
#   - Category descriptions available for logging and debugging
#   - Domain-specific LLM behavior for each category
#
# FALLBACK:
#   - If MCP doesn't return model/use_reasoning, uses default_model below
#   - If MCP doesn't return category_system_prompts, router can use default prompts
#   - Can also add category-specific overrides here if needed
#
categories: []

# Default model to use when category can't be determined
default_model: openai/gpt-oss-20b

# vLLM endpoints configuration
vllm_endpoints:
  - name: endpoint1
    address: 127.0.0.1
    port: 8000
    weight: 1
    health_check_path: /health

# Model-specific configuration
model_config:
  openai/gpt-oss-20b:
    reasoning_family: gpt-oss
    preferred_endpoints:
      - endpoint1
    pii_policy:
      allow_by_default: true

# Reasoning family configurations
reasoning_families:
  deepseek:
    type: chat_template_kwargs
    parameter: thinking
  qwen3:
    type: chat_template_kwargs
    parameter: enable_thinking
  gpt-oss:
    type: reasoning_effort
    parameter: reasoning_effort
  gpt:
    type: reasoning_effort
    parameter: reasoning_effort

# Default reasoning effort level
default_reasoning_effort: high

# Tools configuration (optional)
tools:
  enabled: false
  top_k: 5
  similarity_threshold: 0.7
  tools_db_path: "config/tools_db.json"
  fallback_to_empty: true

# API configuration
api:
  batch_classification:
    max_batch_size: 100
    concurrency_threshold: 5
    max_concurrency: 8
    metrics:
      enabled: true
      detailed_goroutine_tracking: true
      high_resolution_timing: false
      sample_rate: 1.0
      duration_buckets:
        - 0.001
        - 0.005
        - 0.01
        - 0.025
        - 0.05
        - 0.1
        - 0.25
        - 0.5
        - 1
        - 2.5
        - 5
        - 10
        - 30
      size_buckets:
        - 1
        - 2
        - 5
        - 10
        - 20
        - 50
        - 100
        - 200

# Observability configuration
observability:
  tracing:
    enabled: false
    provider: "opentelemetry"
    exporter:
      type: "otlp"
      endpoint: "localhost:4317"
      insecure: true
    sampling:
      type: "always_on"
    resource:
      service_name: "semantic-router"
      service_version: "1.0.0"
      deployment_environment: "production"


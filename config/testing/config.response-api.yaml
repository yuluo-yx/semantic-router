# Response API testing configuration
# Based on config.e2e.yaml with Response API enabled

bert_model:
  model_id: models/mom-embedding-light
  threshold: 0.6
  use_cpu: true

semantic_cache:
  enabled: true
  backend_type: "memory"
  similarity_threshold: 0.8
  max_entries: 1000
  ttl_seconds: 3600

# Response API Configuration - NEW
response_api:
  enabled: true
  store_backend: "memory"  # Use in-memory store for testing
  ttl_seconds: 86400       # 24 hours
  max_responses: 1000

# vLLM Endpoints Configuration
vllm_endpoints:
  - name: "test-endpoint"
    address: "0.0.0.0"
    port: 8000
    weight: 1

model_config:
  "openai/gpt-oss-120b":
    use_reasoning: false
    preferred_endpoints: ["test-endpoint"]

# Minimal classifier configuration
classifier:
  category_model:
    model_id: "models/lora_intent_classifier_bert-base-uncased_model"
    use_modernbert: false
    threshold: 0.6
    use_cpu: true
    category_mapping_path: "models/lora_intent_classifier_bert-base-uncased_model/category_mapping.json"

categories:
  - name: other
    description: "General knowledge and miscellaneous topics"
    mmlu_categories: ["other"]

strategy: "priority"

decisions:
  - name: "default_decision"
    description: "Default catch-all decision"
    priority: 1
    rules:
      operator: "OR"
      conditions:
        - type: "domain"
          name: "other"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: false

default_model: "openai/gpt-oss-120b"

# Reasoning family configurations
reasoning_families:
  qwen3:
    type: "chat_template_kwargs"
    parameter: "enable_thinking"


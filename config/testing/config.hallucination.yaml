# Configuration for testing hallucination detection pipeline
# Tests: fact-check classifier, hallucination detector, and NLI explainer

bert_model:
  model_id: models/all-MiniLM-L12-v2
  threshold: 0.6
  use_cpu: true

semantic_cache:
  enabled: false  # Disable cache for testing

# Classifier configuration
classifier:
  category_model:
    model_id: "models/category_classifier_modernbert-base_model"
    use_modernbert: true
    threshold: 0.6
    use_cpu: true
    category_mapping_path: "models/category_classifier_modernbert-base_model/category_mapping.json"
  pii_model:
    model_id: "models/pii_classifier_modernbert-base_presidio_token_model"
    use_modernbert: true
    threshold: 0.7
    use_cpu: true
    pii_mapping_path: "models/pii_classifier_modernbert-base_presidio_token_model/pii_type_mapping.json"

# Fact-check rules for signal classification
# Similar to keyword_rules and embedding_rules, but based on ML model classification
# The classifier outputs one of these signals that can be referenced in decision conditions
# Threshold is read from hallucination_mitigation.fact_check_model.threshold
fact_check_rules:
  - name: needs_fact_check
    description: "Query contains factual claims that should be verified against context"
  - name: no_fact_check_needed
    description: "Query is creative, code-related, or opinion-based - no fact verification needed"

# Hallucination mitigation model configuration
# Contains model paths for fact-check classifier, hallucination detector, and NLI explainer
hallucination_mitigation:
  # Fact-check classifier: determines if a prompt needs fact verification
  fact_check_model:
    model_id: "models/halugate-sentinel"
    threshold: 0.6
    use_cpu: true

  # Hallucination detector: verifies if LLM response is grounded in context
  hallucination_model:
    model_id: "models/halugate-detector"
    threshold: 0.8
    use_cpu: true

  # NLI model: provides explanations for hallucinated spans
  nli_model:
    model_id: "models/ModernBERT-base-nli"
    threshold: 0.9
    use_cpu: true

# Prompt guard
prompt_guard:
  enabled: true
  use_modernbert: true
  model_id: "models/jailbreak_classifier_modernbert-base_model"
  threshold: 0.7
  use_cpu: true
  jailbreak_mapping_path: "models/jailbreak_classifier_modernbert-base_model/jailbreak_type_mapping.json"

# vLLM endpoints - mock server for testing
vllm_endpoints:
  - name: "mock-vllm"
    address: "127.0.0.1"
    port: 8002
    weight: 1
    health_check_path: "/health"

model_config:
  "qwen3":
    reasoning_family: "qwen3"
    preferred_endpoints: ["mock-vllm"]

# Minimal categories for testing
categories:
  - name: general
    description: "General questions"
    mmlu_categories: ["other"]

strategy: "priority"

decisions:
  - name: "general_decision"
    description: "General questions"
    priority: 100
    rules:
      operator: "OR"
      conditions:
        - type: "domain"
          name: "general"
        - type: "fact_check"
          name: "needs_fact_check"
    modelRefs:
      - model: "qwen3"
        use_reasoning: false
    plugins:
      - type: "hallucination"
        configuration:
          enabled: true
          use_nli: true  # Enable NLI for detailed explanations
          # Action when hallucination detected: "header", "body", "block", or "none"
          hallucination_action: "header"
          # Action when fact-check needed but no tool context: "header", "body", or "none"
          unverified_factual_action: "header"
          # Include detailed info (confidence, spans) in body warning
          include_hallucination_details: true

default_model: qwen3

# API Configuration
api:
  batch_classification:
    metrics:
      enabled: true
      detailed_goroutine_tracking: true
      high_resolution_timing: false
      sample_rate: 1.0


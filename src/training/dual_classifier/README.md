# Dual Classifier System

A dual-purpose neural network classifier using DistilBERT for both **category classification** and **PII (Personally Identifiable Information) detection** with a shared backbone architecture.

## ğŸ¯ Overview

> **ğŸ“ This is Task 2: Proof of Concept (POC) Implementation**  
> **ğŸ”¬ Based on Synthetic Data Generation**

This project implements a **proof-of-concept** dual-task learning system that demonstrates:

### âœ… **Key Accomplishments (Task 2)**
- âœ… **Dual-Purpose Architecture**: Single DistilBERT model (~67M parameters) for both category classification and PII detection
- âœ… **Memory Efficiency**: Shared backbone reduces parameters vs. two separate models
- âœ… **Synthetic Data Pipeline**: Complete data generation with 10 categories and 5 PII pattern types
- âœ… **Training Infrastructure**: Multi-task loss functions, metrics, and optimization
- âœ… **Model Persistence**: Save/load functionality with full state preservation
- âœ… **Comprehensive Testing**: 14 test cases covering all components (100% pass rate)
- âœ… **Performance Validation**: Training completes in ~18.6 seconds on laptop hardware
- âœ… **Production-Ready Features**: Progress tracking, metrics, and model checkpointing

### ğŸ”¬ **POC Characteristics**
- **Data Source**: **Synthetic data generation** (not real-world datasets)
- **Scale**: Small-scale validation (50 training, 20 validation samples)
- **Purpose**: Architecture validation and training pipeline proof
- **Categories**: 10 template-based categories (math, science, history, etc.)
- **PII Patterns**: 5 predefined types (email, phone, SSN, name, address)

### ğŸš€ **Next Steps Roadmap**
- **Task 3**: Real dataset integration (transition from synthetic to production data)
- **Task 4**: Advanced training optimization and scaling
- **Task 5**: Rust implementation with Candle framework

This POC successfully demonstrates that:
- The dual-head architecture works effectively
- Multi-task learning can be implemented efficiently
- The training pipeline is robust and measurable
- Model persistence and loading functions correctly

**Ready for production data integration in Task 3!**

## ğŸ“ File Structure & Descriptions

### Core Model Files

#### `dual_classifier.py`
**Main Model Implementation**
- Contains the `DualClassifier` class built on DistilBERT
- Implements dual-head architecture:
  - **Category Head**: Sequence-level classification for 10 categories
  - **PII Head**: Token-level binary classification for PII detection
- Provides prediction methods for both individual texts and batches
- Includes model save/load functionality (`save_pretrained`, `from_pretrained`)
- **Key Features**: Shared backbone, separate classification heads, integrated tokenizer

#### `trainer.py`
**Training Infrastructure**
- `DualTaskDataset`: PyTorch Dataset class for handling dual-task data
- `DualTaskLoss`: Combined loss function for both classification tasks
- `DualTaskTrainer`: Complete training pipeline with:
  - Training and validation loops
  - Metrics calculation (accuracy for categories, F1-score for PII)
  - Progress tracking with tqdm
  - Model checkpointing and history saving
- **Key Features**: Multi-task loss weighting, gradient accumulation, evaluation metrics

#### `data_generator.py`
**Synthetic Data Generation**
- `SyntheticDataGenerator`: Creates realistic training data
- **Categories**: 10 predefined categories with template texts
- **PII Patterns**: 5 types (email, phone, SSN, name, address)
- **Functions**:
  - `generate_sample()`: Creates single labeled sample
  - `generate_dataset()`: Creates batch datasets with configurable PII ratios
  - `create_sample_datasets()`: Generates train/validation splits
- **Key Features**: Token-level PII labeling, configurable injection rates

### Example & Testing Files

#### `train_example.py`
**Training Demonstration**
- Complete end-to-end training example
- Shows system performance monitoring (CPU, memory, GPU)
- Demonstrates model training with synthetic data
- Includes performance benchmarking and timing
- **Usage**: `python train_example.py`
- **Output**: Trained model saved to `trained_model/` directory

#### `example.py`
**Basic Usage Example**
- Simple demonstration of model usage
- Shows how to:
  - Initialize the DualClassifier
  - Make predictions on text samples
  - Process category and PII results
- **Usage**: `python example.py`
- **Purpose**: Quick start guide for basic model usage

#### `test_existing_model.py`
**Trained Model Validation**
- Tests loading and using a pre-trained model
- Validates that saved models work correctly
- Demonstrates prediction on sample texts
- **Usage**: `python test_existing_model.py`
- **Purpose**: Validate model persistence and loading

### Test Files

#### `test_dual_classifier_system.py`
**Comprehensive Test Suite**
- **14 Test Cases** covering all components:
  - Synthetic data generator functionality
  - Dataset creation and tokenization
  - Loss function computation
  - Training pipeline validation
  - Model save/load functionality
  - End-to-end integration tests
- **Usage**: `python -m pytest test_dual_classifier_system.py -v`
- **Coverage**: All major system components with performance benchmarking

#### `test_dual_classifier.py`
**Core Model Tests**
- Unit tests for the `DualClassifier` class
- Tests model initialization, forward pass, and prediction methods
- Validates tensor shapes and output formats
- **Usage**: `python -m pytest test_dual_classifier.py -v`
- **Focus**: Core model functionality

### Configuration & Dependencies

#### `requirements.txt`
**Project Dependencies**
- **PyTorch**: `>=2.0.0,<=2.2.2` (Neural network backend)
- **Transformers**: `>=4.36.0,<4.45.0` (DistilBERT model)
- **NumPy**: `>=1.24.0,<2.0` (Numerical operations)
- **scikit-learn**: `>=1.0.0` (Evaluation metrics)
- **pytest**: `>=7.0.0` (Testing framework)
- **tqdm**: `>=4.65.0` (Progress bars)
- **datasets**: `>=2.14.0` (Data processing)
- **psutil**: `>=5.9.0` (System monitoring)

#### `DUAL_CLASSIFIER_SYSTEM_TEST_SUMMARY.md`
**Test Results & Documentation**
- Comprehensive testing summary with all results
- Performance benchmarks and system requirements
- Technical achievements and success criteria
- Training results and model metrics
- **Content**: 14/14 tests passed, performance analysis, architecture details

### Model Artifacts

#### `trained_model/` Directory
**Saved Model Files**
- `model.pt` (258MB): Complete trained model state
- `config.json`: Model configuration and hyperparameters
- `training_history.json`: Training metrics and loss curves
- `vocab.txt` (226KB): DistilBERT vocabulary
- `tokenizer_config.json`: Tokenizer configuration
- `special_tokens_map.json`: Special token mappings

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Run Basic Example
```bash
python example.py
```

### 3. Train Your Own Model
```bash
python train_example.py
```

### 4. Test Existing Model
```bash
python test_existing_model.py
```

### 5. Run Full Test Suite
```bash
python -m pytest test_dual_classifier_system.py -v
```

## ğŸ—ï¸ Architecture

### Model Architecture
- **Base Model**: DistilBERT (66M parameters)
- **Total Parameters**: 67,553,292
- **Category Head**: 10-class sequence classification
- **PII Head**: Token-level binary classification
- **Shared Backbone**: Memory-efficient design

### Training Pipeline
- **Multi-task Loss**: Weighted combination of category and PII losses
- **Metrics**: Category accuracy and PII F1-score
- **Data**: Synthetic generation with configurable PII injection
- **Optimization**: Adam optimizer with learning rate scheduling

## ğŸ“Š Performance

### Training Performance
- **Training Time**: ~18.6 seconds for 50 samples
- **System Requirements**: 8-core CPU, 16GB RAM (no GPU required)
- **Memory Efficiency**: Single model vs. two separate models

### Model Performance
- **Category Accuracy**: 45% (on small synthetic dataset)
- **PII F1-Score**: 91.09%
- **Training Loss**: 1.4948 (final)
- **Validation Loss**: 1.5169 (final)

## ğŸ§ª Testing

The project includes comprehensive testing with 14 test cases covering:
- âœ… Synthetic data generation
- âœ… Dataset creation and tokenization
- âœ… Loss function computation
- âœ… Training pipeline validation
- âœ… Model persistence
- âœ… End-to-end integration

All tests pass with excellent performance ratings.

## ğŸ“ˆ Next Steps

This implementation provides a foundation for:
- **Task 3**: Real dataset integration
- **Task 4**: Advanced training optimization
- **Task 5**: Rust implementation with Candle framework

## ğŸ¤ Usage Examples

### Basic Prediction
```python
from dual_classifier import DualClassifier

# Load model
model = DualClassifier.from_pretrained('trained_model/', num_categories=10)

# Make prediction
text = "What is the derivative of x^2?"
category_probs, pii_probs = model.predict(text)
```

### Visual Example: How Dual Prediction Works ğŸ¯

Here's a detailed example showing how the model processes text for **both** category classification and PII detection simultaneously:

```python
from dual_classifier import DualClassifier

# Initialize model
model = DualClassifier(num_categories=10)
text = "My email is john@example.com. What is calculus?"

# Step 1: encode_text() - Text Preprocessing Only ğŸ”¤
encoded = model.encode_text(text)
# Result: {"input_ids": [101, 2026, 4183, 2003, 2198, 1030, 2742, 1012, 4012, 102], 
#          "attention_mask": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}

# Step 2: predict() - Does BOTH Tasks Simultaneously ğŸš€
category_probs, pii_probs = model.predict(text)

# Category Classification Results:
# category_probs shape: (1, 10) - probabilities for 10 categories
# Example output: [0.05, 0.82, 0.03, 0.02, 0.01, 0.02, 0.01, 0.02, 0.01, 0.01]
#                  â†‘     â†‘
#                 cat0  cat1 (math) - highest probability!

print(f"Predicted category: {torch.argmax(category_probs[0]).item()}")  # Output: 1 (math)
print(f"Confidence: {category_probs[0][1].item():.3f}")                 # Output: 0.820

# PII Detection Results:
# pii_probs shape: (1, sequence_length, 2) - PII probability for each token
# Each token gets [prob_not_pii, prob_is_pii]

tokens = model.tokenizer.tokenize(text)
pii_predictions = torch.argmax(pii_probs[0], dim=-1)

print("Token-level PII detection:")
for token, pred in zip(tokens, pii_predictions):
    pii_status = "ğŸ”’ PII" if pred == 1 else "âœ… Safe"
    print(f"  '{token}' â†’ {pii_status}")

# Expected output:
#   'my' â†’ âœ… Safe
#   'email' â†’ âœ… Safe  
#   'is' â†’ âœ… Safe
#   'john' â†’ ğŸ”’ PII
#   '@' â†’ ğŸ”’ PII
#   'example' â†’ ğŸ”’ PII
#   '.' â†’ ğŸ”’ PII
#   'com' â†’ ğŸ”’ PII
#   'what' â†’ âœ… Safe
#   'is' â†’ âœ… Safe
#   'calculus' â†’ âœ… Safe
```

**Key Points:**
- ğŸ“ **Single Input, Dual Output**: One text â†’ category + PII results simultaneously
- ğŸ”„ **`encode_text()`**: Just preprocessing, no predictions
- ğŸ¯ **`predict()`**: Does BOTH tasks at once using shared DistilBERT backbone
- ğŸ§  **Memory Efficient**: Single model handles both tasks vs. separate models

### Training New Model
```python
from trainer import DualTaskTrainer
from data_generator import create_sample_datasets

# Generate data
train_dataset, val_dataset = create_sample_datasets()

# Train model
trainer = DualTaskTrainer(model, train_dataset, val_dataset)
trainer.train(num_epochs=2)
``` 
# vLLM Semantic Router
# Users only configure: version, listeners, signals, decisions, llm
# System defaults (semantic_cache, classifier, etc.) are provided by CLI

version: v0.1

# Listeners - Network configuration
listeners:
  - name: "http-8888"
    address: "0.0.0.0"
    port: 8888
    timeout: "300s"

# Signals - Classification signals for routing
signals:
  # Keyword-based signals
  keywords:
    - name: "math_keywords"
      operator: "OR"
      keywords:
        - "calculate"
        - "equation"
      case_sensitive: false

    - name: "code_keywords"
      operator: "OR"
      keywords:
        - "function"
        - "class"
      case_sensitive: false

    - name: "looper_keywords"
      operator: "OR"
      keywords:
        - "looper"
        - "collaborative"
      case_sensitive: false

  # Embedding-based signals (semantic similarity)
  embeddings:
    - name: "quick_answer"
      threshold: 0.70
      candidates:
        - "quick answer"
        - "simple question"
        - "fast response"
        - "brief reply"
        - "short answer"
        - "just tell me"
        - "answer quickly"
        - "give me the result"
      aggregation_method: "max"

    - name: "deep_thinking"
      threshold: 0.75
      candidates:
        - "think step by step"
        - "reason through this"
        - "analyze carefully"
        - "deep thinking"
        - "think thoroughly"
        - "consider all aspects"
        - "break down the problem"
        - "explain your reasoning"
        - "show logical steps"
        - "think systematically"
        - "analyze in detail"
        - "work through methodically"
        - "use critical thinking"
        - "apply logical reasoning"
        - "think deeply"
        - "consider carefully"
        - "examine thoroughly"
        - "reason systematically"
        - "analyze step by step"
        - "think it through"
        - "evaluate carefully"
        - "assess thoroughly"
        - "think critically"
        - "reason logically"
        - "analyze comprehensively"
        - "think methodically"
        - "examine carefully"
        - "consider systematically"
        - "think rationally"
        - "analyze rigorously"
      aggregation_method: "max"

  # domains - Domain categories (optional, can be auto-generated)
  domains:
    - name: "math"
      description: "Mathematics and quantitative reasoning"
      mmlu_categories: ["math"]
    - name: "physics"
      description: "Physics and physical sciences"
      mmlu_categories: ["physics"]
    - name: "chemistry"
      description: "Chemistry and chemical sciences questions"
      mmlu_categories: ["chemistry"]
    - name: "biology"
      description: "Biology and life sciences questions"
      mmlu_categories: ["biology"]
    - name: "computer_science"
      description: "Computer science and programming"
      mmlu_categories: ["computer_science"]
    - name: "engineering"
      description: "Engineering and technical problem-solving"
      mmlu_categories: ["engineering"]
    - name: "business"
      description: "Business and management related queries"
      mmlu_categories: ["business"]
    - name: "economics"
      description: "Economics and financial topics"
      mmlu_categories: ["economics"]
    - name: "law"
      description: "Legal questions and law-related topics"
      mmlu_categories: ["law"]
    - name: "psychology"
      description: "Psychology and mental health topics"
      mmlu_categories: ["psychology"]
    - name: "philosophy"
      description: "Philosophy and ethical questions"
      mmlu_categories: ["philosophy"]
    - name: "history"
      description: "Historical questions and cultural topics"
      mmlu_categories: ["history"]
    - name: "health"
      description: "Health and medical information queries"
      mmlu_categories: ["health"]
    - name: "other"
      description: "General knowledge and miscellaneous topics"
      mmlu_categories: ["other"]

  # fact_check - Fact-checking signals
  fact_check:
    - name: needs_fact_check
      description: "Query contains factual claims that should be verified against context"
    - name: no_fact_check_needed
      description: "Query is creative, code-related, or opinion-based - no fact verification needed"

  # user_feedbacks - User feedback signals
  user_feedbacks:
    - name: need_clarification
      description: "User needs more clarification on the answer"
    - name: satisfied
      description: "User is satisfied with the answer"
    - name: want_different
      description: "User wants some other different answer"
    - name: wrong_answer
      description: "User is not satisfied with the answer and it is wrong"

  # preferences - Route preferences via external LLM
  preferences:
    - name: "code_generation"
      description: "Generating new code snippets, writing functions, creating classes"
    - name: "bug_fixing"
      description: "Identifying and fixing errors, debugging issues, troubleshooting problems"
    - name: "code_review"
      description: "Reviewing code quality, suggesting improvements, best practices"
    - name: "other"
      description: "Irrelevant queries or already fulfilled requests"

  # language - Multi-language detection signals
  language:
    - name: "en"
      description: "English language queries"
    - name: "es"
      description: "Spanish language queries"
    - name: "zh"
      description: "Chinese language queries"
    - name: "fr"
      description: "French language queries"
    - name: "ru"
      description: "Russian language queries"
    - name: "de"
      description: "German language queries"
    - name: "ja"
      description: "Japanese language queries"

  # latency - Latency-based routing signals (TPOT-based)
  latency:
    - name: "low_latency"
      max_tpot: 0.05  # 50ms per token
      description: "For real-time chat applications requiring fast responses"
    - name: "medium_latency"
      max_tpot: 0.15  # 150ms per token
      description: "For standard applications with moderate latency tolerance"

  # context_rules - Context length signals (Token Count)
  context_rules:
    - name: "low_token_count"
      min_tokens: "0"
      max_tokens: "1K"
      description: "Short requests"
    - name: "high_token_count"
      min_tokens: "1K"
      max_tokens: "128K"
      description: "Long requests requiring large context window"

# Decisions - Routing logic
decisions:
  # Highest priority: Preference-based routing via external LLM
  - name: "preference_code_generation"
    description: "Route code generation requests based on LLM preference matching"
    priority: 200
    rules:
      operator: "AND"
      conditions:
        - type: "preference"
          name: "code_generation"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: false
    plugins:
      - type: "system_prompt"
        configuration:
          system_prompt: "You are an expert code generator. Write clean, efficient, and well-documented code."

  - name: "preference_bug_fixing"
    description: "Route bug fixing requests based on LLM preference matching"
    priority: 200
    rules:
      operator: "AND"
      conditions:
        - type: "preference"
          name: "bug_fixing"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: true
    plugins:
      - type: "system_prompt"
        configuration:
          system_prompt: "You are an expert debugger. Analyze the issue carefully, identify the root cause, and provide a clear fix with explanation."

  # High priority: Quick answer requests
  - name: "quick_answer_route"
    description: "Route quick answer requests to fast model without reasoning"
    priority: 180
    rules:
      operator: "AND"
      conditions:
        - type: "embedding"
          name: "quick_answer"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: true
        reasoning_effort: "low"
    plugins:
      - type: "system_prompt"
        configuration:
          system_prompt: "Provide a concise and direct answer. Be brief and to the point."

  # High priority: Deep thinking requests
  - name: "deep_thinking_route"
    description: "Route deep thinking requests with reasoning enabled"
    priority: 180
    rules:
      operator: "AND"
      conditions:
        - type: "embedding"
          name: "deep_thinking"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: true
        reasoning_effort: "high"
    plugins:
      - type: "system_prompt"
        configuration:
          system_prompt: "Think deeply and systematically. Break down the problem, analyze each step carefully, and provide a thorough explanation of your reasoning process."

  # Language-based routing
  - name: "russian_route"
    description: "Route Russian queries to Russian-optimized model"
    priority: 80
    rules:
      operator: "AND"
      conditions:
        - type: "language"
          name: "ru"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: false

  - name: "chinese_route"
    description: "Route Chinese queries to Chinese-optimized model"
    priority: 80
    rules:
      operator: "AND"
      conditions:
        - type: "language"
          name: "zh"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: false

  - name: "math_problems"
    description: "Route mathematical queries with reasoning enabled"
    priority: 100
    rules:
      operator: "OR"
      conditions:
        - type: "keyword"
          name: "math_keywords"
        - type: "domain"
          name: "math"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: true
        reasoning_effort: "high"  # Override default reasoning effort for math queries
    plugins:
      - type: "system_prompt"
        configuration:
          system_prompt: "You are a mathematics expert. Provide step-by-step solutions with clear explanations."
      - type: "semantic-cache"
        configuration:
          enabled: true
          similarity_threshold: 0.92
      # Router Replay plugin: Capture routing decisions for debugging
      # Records accessible via /v1/router_replay API endpoint
      - type: "router_replay"
        configuration:
          enabled: true
          max_records: 200              # Maximum records in memory (default: 200)
          capture_request_body: true   # Capture request payloads (default: false)
          capture_response_body: false  # Capture response payloads (default: false)
          max_body_bytes: 4096          # Max bytes per body, truncates if exceeded (default: 4096)

  - name: "physics_problems"
    description: "Route physics queries with reasoning"
    priority: 100
    rules:
      operator: "AND"
      conditions:
        - type: "domain"
          name: "physics"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: true
        reasoning_effort: "medium"  # Use medium effort for physics queries
    plugins:
      - type: "system_prompt"
        configuration:
          system_prompt: "You are a physics expert. Explain concepts clearly with mathematical derivations."

  - name: "code_route"
    description: "Route programming queries"
    priority: 100
    rules:
      operator: "OR"
      conditions:
        - type: "keyword"
          name: "code_keywords"
        - type: "domain"
          name: "computer_science"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: false
    plugins:
      - type: "system_prompt"
        configuration:
          system_prompt: "You are a programming expert. Provide clear code examples."

  - name: "general_route"
    description: "Default fallback route for general queries"
    priority: 50
    rules:
      operator: "AND"
      conditions:
        - type: "domain"
          name: "other"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: false
    plugins:
      - type: "semantic-cache"
        configuration:
          enabled: true
          similarity_threshold: 0.85

  # Size-aware routing example: Try smaller models first, escalate if confidence is low
  - name: "confidence_route"
    description: "Cost-efficient routing: try small model first, escalate if needed"
    priority: 40
    rules:
      operator: "AND"
      conditions:
        - type: "domain"
          name: "other"
        - type: "keyword"
          name: "looper_keywords"
    modelRefs:
      - model: "openai/gpt-oss-120b"    # param_size defined in providers.models
      - model: "gpt-5.2"
    algorithm:
      type: "confidence"
      confidence:
        # Confidence evaluation method (all normalized to 0-1, higher = more confident):
        # - "avg_logprob": Based on average token logprob
        # - "margin": Based on margin between top-1 and top-2 tokens
        # - "hybrid": Weighted combination of both
        confidence_method: "margin"
        threshold: 0.9      # Escalate to larger model if confidence < threshold (0-1)
        # For hybrid method, configure weights:
        # hybrid_weights:
        #   logprob_weight: 0.4
        #   margin_weight: 0.6
        on_error: "skip"    # Skip failed models, try next

  # Ratings algorithm example: Use multiple models, select best based on ratings
  - name: "ratings_route"
    description: "Route using ratings algorithm for model selection"
    priority: 60
    rules:
      operator: "OR"
      conditions:
        - type: "keyword"
          name: "looper_keywords"
        - type: "domain"
          name: "other"
    modelRefs:
      - model: "openai/gpt-oss-120b"    # param_size defined in providers.models
      - model: "gpt-5.2"
    algorithm:
      type: "ratings"
      ratings:
        on_error: "skip"    # Skip failed models, try next
    plugins:
      - type: "semantic-cache"
        configuration:
          enabled: false

# LLM - Backend model configuration
providers:
  # Model configuration
  # param_size is used by confidence algorithm to sort models (smallest first)
  models:
    - name: "openai/gpt-oss-120b"
      reasoning_family: "gpt-oss"
      param_size: "120b"  # Used for confidence routing
      endpoints:
        - name: "vllm_endpoint"
          weight: 1
          endpoint: "host.docker.internal:8000"
          protocol: "http"
      access_key: "jGNm/R0CpwDQsvJoaABrymRTrGKt7B63dikib7hW8EyMISRJr"
    - name: "gpt-5.2"
      param_size: "330b"  # Used for confidence routing
      endpoints:
        - name: "external_provider"
          weight: 1
          endpoint: "api.openai.com"
          protocol: "https"
      access_key: "sk-xxxxxx"

  # Default model for fallback
  default_model: "openai/gpt-oss-120b"

  # Reasoning families configuration
  reasoning_families:
    gpt-oss:
      type: "reasoning_effort"
      parameter: "reasoning_effort"

  # Default reasoning effort level
  default_reasoning_effort: "low"

# vLLM Semantic Router
# Users only configure: version, listeners, signals, decisions, llm
# System defaults (semantic_cache, classifier, etc.) are provided by CLI

version: v0.1

# Listeners - Network configuration
listeners:
  - name: "http-8888"
    address: "0.0.0.0"
    port: 8888
    timeout: "300s"

# Signals - Classification signals for routing
signals:
  # Keyword-based signals
  keywords:
    - name: "math_keywords"
      operator: "OR"
      keywords:
        - "calculate"
        - "equation"
      case_sensitive: false

    - name: "code_keywords"
      operator: "OR"
      keywords:
        - "function"
        - "class"
      case_sensitive: false

    - name: "looper_keywords"
      operator: "OR"
      keywords:
        - "looper"
        - "collaborative"
      case_sensitive: false

  # Embedding-based signals (semantic similarity)
  embeddings:
    - name: "code_debug"
      threshold: 0.70
      candidates:
        - "how to debug the code"
        - "i meet a problem with my code"
        - "need to troubleshooting steps for my code"
      aggregation_method: "max"

  # domains - Domain categories (optional, can be auto-generated)
  domains:
    - name: "math"
      description: "Mathematics and quantitative reasoning"
      mmlu_categories: ["math"]

    - name: "computer_science"
      description: "Programming and computer science"
      mmlu_categories: ["computer_science"]

    - name: "physics"
      description: "Physics and physical sciences"
      mmlu_categories: ["physics"]

    - name: "other"
      description: "General queries"
      mmlu_categories: ["other"]

  # fact_check - Fact-checking signals
  fact_check:
    - name: needs_fact_check
      description: "Query contains factual claims that should be verified against context"
    - name: no_fact_check_needed
      description: "Query is creative, code-related, or opinion-based - no fact verification needed"

  # user_feedbacks - User feedback signals
  user_feedbacks:
    - name: need_clarification
      description: "User needs more clarification on the answer"
    - name: satisfied
      description: "User is satisfied with the answer"
    - name: want_different
      description: "User wants some other different answer"
    - name: wrong_answer
      description: "User is not satisfied with the answer and it is wrong"

  # preferences - Route preferences via external LLM
  preferences:
    - name: "code_generation"
      description: "Generating new code snippets, writing functions, creating classes"
    - name: "bug_fixing"
      description: "Identifying and fixing errors, debugging issues, troubleshooting problems"
    - name: "code_review"
      description: "Reviewing code quality, suggesting improvements, best practices"
    - name: "other"
      description: "Irrelevant queries or already fulfilled requests"

  # language - Multi-language detection signals
  language:
    - name: "en"
      description: "English language queries"
    - name: "es"
      description: "Spanish language queries"
    - name: "ru"
      description: "Russian language queries"
    - name: "zh"
      description: "Chinese language queries"
    - name: "fr"
      description: "French language queries"
    - name: "de"
      description: "German language queries"
    - name: "ja"
      description: "Japanese language queries"

# Decisions - Routing logic
decisions:
  # Highest priority: Preference-based routing via external LLM
  - name: "preference_code_generation"
    description: "Route code generation requests based on LLM preference matching"
    priority: 200
    rules:
      operator: "AND"
      conditions:
        - type: "preference"
          name: "code_generation"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: false
    plugins:
      - type: "system_prompt"
        configuration:
          system_prompt: "You are an expert code generator. Write clean, efficient, and well-documented code."

  - name: "preference_bug_fixing"
    description: "Route bug fixing requests based on LLM preference matching"
    priority: 200
    rules:
      operator: "AND"
      conditions:
        - type: "preference"
          name: "bug_fixing"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: true
    plugins:
      - type: "system_prompt"
        configuration:
          system_prompt: "You are an expert debugger. Analyze the issue carefully, identify the root cause, and provide a clear fix with explanation."

  # High priority: Handle user feedback for wrong answers
  - name: "wrong_answer_route"
    description: "Handle user feedback indicating wrong answer - rethink and provide correct response"
    priority: 150
    rules:
      operator: "AND"
      conditions:
        - type: "user_feedback"
          name: "wrong_answer"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: true
    plugins:
      - type: "system_prompt"
        configuration:
          system_prompt: "The user has indicated that the previous answer was incorrect. Please carefully reconsider the question, identify what might have been wrong in the previous response, and provide a corrected and accurate answer. Think step-by-step and verify your reasoning before responding."

  # Language-based routing
  - name: "russian_route"
    description: "Route Russian queries to Russian-optimized model"
    priority: 80
    rules:
      operator: "AND"
      conditions:
        - type: "language"
          name: "ru"
    modelRefs:
      - model: "russian-model"
        use_reasoning: false

  - name: "chinese_route"
    description: "Route Chinese queries to Chinese-optimized model"
    priority: 80
    rules:
      operator: "AND"
      conditions:
        - type: "language"
          name: "zh"
    modelRefs:
      - model: "chinese-model"
        use_reasoning: false

  - name: "math_route"
    description: "Route mathematical queries with reasoning enabled"
    priority: 100
    rules:
      operator: "OR"
      conditions:
        - type: "keyword"
          name: "math_keywords"
        - type: "domain"
          name: "math"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: true
    plugins:
      - type: "system_prompt"
        configuration:
          system_prompt: "You are a mathematics expert. Provide step-by-step solutions with clear explanations."
      - type: "semantic-cache"
        configuration:
          enabled: true
          similarity_threshold: 0.92

  - name: "physics_route"
    description: "Route physics queries with reasoning"
    priority: 100
    rules:
      operator: "AND"
      conditions:
        - type: "domain"
          name: "physics"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: true
    plugins:
      - type: "system_prompt"
        configuration:
          system_prompt: "You are a physics expert. Explain concepts clearly with mathematical derivations."

  - name: "code_route"
    description: "Route programming queries"
    priority: 100
    rules:
      operator: "OR"
      conditions:
        - type: "keyword"
          name: "code_keywords"
        - type: "domain"
          name: "computer_science"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: false
    plugins:
      - type: "system_prompt"
        configuration:
          system_prompt: "You are a programming expert. Provide clear code examples."

  - name: "general_route"
    description: "Default fallback route for general queries"
    priority: 50
    rules:
      operator: "AND"
      conditions:
        - type: "domain"
          name: "other"
    modelRefs:
      - model: "openai/gpt-oss-120b"
        use_reasoning: false
    plugins:
      - type: "semantic-cache"
        configuration:
          enabled: true
          similarity_threshold: 0.85

  # Size-aware routing example: Try smaller models first, escalate if confidence is low
  - name: "confidence_route"
    description: "Cost-efficient routing: try small model first, escalate if needed"
    priority: 40
    rules:
      operator: "AND"
      conditions:
        - type: "domain"
          name: "other"
        - type: "keyword"
          name: "looper_keywords"
    modelRefs:
      - model: "openai/gpt-oss-120b"    # param_size defined in providers.models
      - model: "openai/gpt-5.2"
    algorithm:
      type: "confidence"
      confidence:
        # Confidence evaluation method (all normalized to 0-1, higher = more confident):
        # - "avg_logprob": Based on average token logprob
        # - "margin": Based on margin between top-1 and top-2 tokens
        # - "hybrid": Weighted combination of both
        confidence_method: "margin"
        threshold: 0.9      # Escalate to larger model if confidence < threshold (0-1)
        # For hybrid method, configure weights:
        # hybrid_weights:
        #   logprob_weight: 0.4
        #   margin_weight: 0.6
        on_error: "skip"    # Skip failed models, try next

  # Ratings algorithm example: Use multiple models, select best based on ratings
  - name: "ratings_route"
    description: "Route using ratings algorithm for model selection"
    priority: 60
    rules:
      operator: "OR"
      conditions:
        - type: "keyword"
          name: "looper_keywords"
        - type: "domain"
          name: "other"
    modelRefs:
      - model: "openai/gpt-oss-120b"    # param_size defined in providers.models
      - model: "openai/gpt-5.2"
    algorithm:
      type: "ratings"
      ratings:
        on_error: "skip"    # Skip failed models, try next
    plugins:
      - type: "semantic-cache"
        configuration:
          enabled: false

# LLM - Backend model configuration
providers:
  # Model configuration
  # param_size is used by confidence algorithm to sort models (smallest first)
  models:
    - name: "openai/gpt-oss-120b"
      reasoning_family: "gpt-oss"
      param_size: "120b"  # Used for confidence routing
      endpoints:
        - name: "vllm_endpoint"
          weight: 1
          endpoint: "host.docker.internal:8000"
          protocol: "http"
      access_key: "jGNm/R0CpwDQsvJoaABrymRTrGKt7B63dikib7hW8EyMISRJr"
    - name: "openai/gpt-5.2"
      param_size: "330b"  # Used for confidence routing
      endpoints:
        - name: "external_provider"
          weight: 1
          endpoint: "api.openai.com"
          protocol: "https"
      access_key: "sk-xxxxxx"

  # Default model for fallback
  default_model: "openai/gpt-oss-120b"

  # Reasoning families configuration
  reasoning_families:
    gpt-oss:
      type: "reasoning_effort"
      parameter: "reasoning_effort"

  # Default reasoning effort level
  default_reasoning_effort: "high"

# Semantic Router Configuration for Dynamic Config E2E Testing
# This configuration uses Kubernetes CRDs for dynamic configuration
# Static parts are defined here, dynamic parts (model_config, decisions, categories) come from CRDs

# Environment variables for the semantic-router container
env:
  - name: EMBEDDING_MODEL_OVERRIDE
    value: "qwen3"  # Force qwen3 for tests (Gemma requires HF_TOKEN)

config:
  # Set config source to kubernetes to enable CRD-based configuration
  config_source: kubernetes

  # Static configuration - these are not managed by CRDs

  bert_model:
    model_id: models/all-MiniLM-L12-v2
    threshold: 0.6
    use_cpu: true

  semantic_cache:
    enabled: true
    backend_type: "memory"
    similarity_threshold: 0.8
    max_entries: 1000
    ttl_seconds: 3600
    eviction_policy: "fifo"
    use_hnsw: true
    hnsw_m: 16
    hnsw_ef_construction: 200
    embedding_model: "bert"

  tools:
    enabled: true
    top_k: 3
    similarity_threshold: 0.2
    tools_db_path: "config/tools_db.json"
    fallback_to_empty: true

  prompt_guard:
    enabled: true
    use_modernbert: true
    model_id: "models/jailbreak_classifier_modernbert-base_model"
    threshold: 0.7
    use_cpu: true
    jailbreak_mapping_path: "models/jailbreak_classifier_modernbert-base_model/jailbreak_type_mapping.json"

  classifier:
    category_model:
      model_id: "models/category_classifier_modernbert-base_model"
      use_modernbert: true
      threshold: 0.6
      use_cpu: true
      category_mapping_path: "models/category_classifier_modernbert-base_model/category_mapping.json"
    pii_model:
      model_id: "models/lora_pii_detector_bert-base-uncased_model"
      use_modernbert: false  # Use LoRA PII model
      threshold: 0.9
      use_cpu: true
      pii_mapping_path: "models/pii_classifier_modernbert-base_presidio_token_model/pii_type_mapping.json"

  router:
    high_confidence_threshold: 0.99
    low_latency_threshold_ms: 2000
    lora_baseline_score: 0.8
    traditional_baseline_score: 0.7
    embedding_baseline_score: 0.75
    success_confidence_threshold: 0.8
    large_batch_threshold: 4
    lora_default_execution_time_ms: 1345
    traditional_default_execution_time_ms: 4567
    default_confidence_threshold: 0.95
    default_max_latency_ms: 5000
    default_batch_size: 4
    default_avg_execution_time_ms: 3000
    lora_default_confidence: 0.99
    traditional_default_confidence: 0.95
    lora_default_success_rate: 0.98
    traditional_default_success_rate: 0.95
    multi_task_lora_weight: 0.30
    single_task_traditional_weight: 0.30
    large_batch_lora_weight: 0.25
    small_batch_traditional_weight: 0.25
    medium_batch_weight: 0.10
    high_confidence_lora_weight: 0.25
    low_confidence_traditional_weight: 0.25
    low_latency_lora_weight: 0.30
    high_latency_traditional_weight: 0.10
    performance_history_weight: 0.20
    traditional_bert_confidence_threshold: 0.95
    traditional_modernbert_confidence_threshold: 0.8
    traditional_pii_detection_threshold: 0.5
    traditional_token_classification_threshold: 0.9
    traditional_dropout_prob: 0.1
    traditional_attention_dropout_prob: 0.1
    tie_break_confidence: 0.5

  reasoning_families:
    deepseek:
      type: "chat_template_kwargs"
      parameter: "thinking"
    qwen3:
      type: "chat_template_kwargs"
      parameter: "enable_thinking"
    gpt-oss:
      type: "reasoning_effort"
      parameter: "reasoning_effort"
    gpt:
      type: "reasoning_effort"
      parameter: "reasoning_effort"

  default_reasoning_effort: high

  api:
    batch_classification:
      max_batch_size: 100
      concurrency_threshold: 5
      max_concurrency: 8
      metrics:
        enabled: true
        detailed_goroutine_tracking: true
        high_resolution_timing: false
        sample_rate: 1.0
        duration_buckets:
          [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10, 30]
        size_buckets: [1, 2, 5, 10, 20, 50, 100, 200]

  embedding_models:
    qwen3_model_path: "models/Qwen3-Embedding-0.6B"
    gemma_model_path: ""  # Empty = fallback to Qwen3 (embeddinggemma requires HF_TOKEN)
    use_cpu: true

# Increase memory limits for embedding model support
resources:
  limits:
    memory: "10Gi"  # Increased from default 6Gi to handle Qwen3 + all classification models
    cpu: "2"
  requests:
    memory: "6Gi"   # Increased from default 3Gi
    cpu: "1"

  observability:
    tracing:
      enabled: false
      provider: "opentelemetry"
      exporter:
        type: "otlp"
        endpoint: "jaeger:4317"
        insecure: true
      sampling:
        type: "always_on"
        rate: 1.0
      resource:
        service_name: "vllm-semantic-router"
        service_version: "v0.1.0"
        deployment_environment: "development"

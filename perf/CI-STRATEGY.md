# Performance Testing CI Strategy

## The Problem You Identified

Running performance tests on **every PR** has significant costs:

- ðŸ’¸ **Cost:** Burns 15-20 CI minutes per PR
- ðŸŒ **Speed:** Slows down developer workflow
- ðŸ“Š **Noise:** CI variance causes false positives
- ðŸ”¥ **Resources:** Downloads models, uses CPU intensively

**You're right to question this!**

---

## Current Setup (After Optimization)

The workflow now runs **only when needed**:

### âœ… Performance Tests Run When:

1. **PR has `performance` label** â† Developer explicitly requests it
2. **Manual trigger** â† Via GitHub Actions UI
3. ~~Every PR~~ â† **REMOVED to save costs**

### Usage:

```bash
# Developer workflow:
1. Open PR with code changes
2. Regular tests run (fast)
3. If touching performance-critical code:
   â†’ Add "performance" label to PR
   â†’ Performance tests run automatically
4. Review results in PR comment
```

---

## Alternative Strategies

Here are different approaches teams use, from most to least restrictive:

### Strategy 1: Label-Based (CURRENT - RECOMMENDED) ðŸ·ï¸

**When it runs:**

- Only when PR has `performance` label
- Manual trigger via GitHub UI

**Pros:**

- âœ… Saves tons of CI time
- âœ… Developers control when tests run
- âœ… No noise on small PRs

**Cons:**

- âŒ Developers might forget to add label
- âŒ Regressions could slip through

**Best for:** Most teams, cost-conscious projects

---

### Strategy 2: Path-Based (Original Design) ðŸ“

**When it runs:**

```yaml
on:
  pull_request:
    paths:
      - 'src/semantic-router/**'
      - 'candle-binding/**'
      - 'perf/**'
```

**Pros:**

- âœ… Automatic - no manual intervention
- âœ… Catches regressions early

**Cons:**

- âŒ Runs too often (most PRs touch these paths)
- âŒ High CI cost
- âŒ Slows down development

**Best for:** Critical production systems, unlimited CI budget

---

### Strategy 3: Scheduled + Manual Only â°

**When it runs:**

```yaml
on:
  schedule:
    - cron: "0 2 * * *"  # Daily at 2 AM
  workflow_dispatch:      # Manual only
```

**Pros:**

- âœ… Minimal CI cost
- âœ… No PR delays
- âœ… Nightly baseline still updates

**Cons:**

- âŒ Regressions found after merge (too late!)
- âŒ Developers must manually trigger

**Best for:** Early-stage projects, limited resources

---

### Strategy 4: Hybrid - Critical Paths Only ðŸŽ¯

**When it runs:**

```yaml
on:
  pull_request:
    paths:
      - 'src/semantic-router/pkg/classification/**'  # Critical
      - 'src/semantic-router/pkg/cache/**'           # Critical
      - 'candle-binding/**'                          # Critical
      # NOT: docs, tests, configs, etc.
```

**Pros:**

- âœ… Automatic for critical code
- âœ… Reduced CI usage vs path-based
- âœ… Catches most important regressions

**Cons:**

- âŒ Still runs frequently
- âŒ Can miss indirect performance impacts

**Best for:** Mature projects with clear critical paths

---

### Strategy 5: PR Size Based ðŸ“

**When it runs:**

```yaml
# Run only on large PRs (>500 lines changed)
if: github.event.pull_request.additions + github.event.pull_request.deletions > 500
```

**Pros:**

- âœ… Small PRs skip expensive tests
- âœ… Large risky changes get tested

**Cons:**

- âŒ Single-line change can cause regression
- âŒ Complex logic to maintain

**Best for:** Teams with predictable PR sizes

---

### Strategy 6: Pre-merge Only (Protected Branch) ðŸ”’

**When it runs:**

```yaml
on:
  pull_request:
    types: [ready_for_review]  # Only when marked ready
  # OR
  push:
    branches: [main]  # Only after merge
```

**Pros:**

- âœ… Tests final code before/after merge
- âœ… Doesn't slow down draft PRs

**Cons:**

- âŒ Late feedback for developers
- âŒ Might catch issues post-merge

**Best for:** Fast-moving teams, trust-based workflows

---

## Recommended Setup by Project Stage

### ðŸŒ± Early Stage Project

```yaml
Strategy: Scheduled + Manual
Performance Tests: Nightly only
Reason: Save CI budget, iterate fast
```

### ðŸŒ¿ Growing Project

```yaml
Strategy: Label-Based (CURRENT)
Performance Tests: On 'performance' label
Reason: Balance cost vs safety
```

### ðŸŒ³ Mature Project

```yaml
Strategy: Hybrid Critical Paths
Performance Tests: Auto on critical code
Reason: High confidence, catch regressions
```

### ðŸ¢ Enterprise Project

```yaml
Strategy: Every PR (Path-Based)
Performance Tests: Always
Reason: Zero tolerance for regressions
```

---

## How to Switch Strategies

### Switch to "Every PR" (Path-Based)

```yaml
# .github/workflows/performance-test.yml
on:
  pull_request:
    branches: [main]
    paths:
      - 'src/semantic-router/**'
      - 'candle-binding/**'

jobs:
  component-benchmarks:
    runs-on: ubuntu-latest
    # Remove the check-should-run job
    # Remove the needs/if conditions
```

### Switch to "Nightly Only"

```yaml
# .github/workflows/performance-test.yml
on:
  schedule:
    - cron: "0 3 * * *"
  workflow_dispatch:

# Disable PR trigger completely
```

### Keep Current (Label-Based)

No changes needed! Current setup is optimized.

---

## Cost Analysis

Assuming:

- 10 PRs per day
- 20 minutes per performance test
- $0.008 per minute (GitHub Actions pricing)

| Strategy | PRs Tested | CI Minutes/Day | Cost/Month |
|----------|------------|----------------|------------|
| Every PR | 10 | 200 min | $48/month |
| Label (25% use) | 2.5 | 50 min | $12/month |
| Critical Paths | 5 | 100 min | $24/month |
| Nightly Only | 0 | 0 min | $0/month |

**Current Label-Based:** Saves ~$36/month vs Every PR! ðŸ’°

---

## Best Practices

### For Developers

**When to add `performance` label:**

- âœ… Changing classification, cache, or decision engine
- âœ… Modifying CGO bindings
- âœ… Optimizing algorithms
- âœ… Changing batch processing logic
- âŒ Updating docs or tests
- âŒ Fixing typos
- âŒ Changing configs

### For Reviewers

**Check for performance label:**

```markdown
## Performance Checklist
- [ ] Does this PR touch classification/cache/decision code?
- [ ] Could this impact request latency?
- [ ] Should we add 'performance' label and run tests?
```

### For CI

**Monitor false negatives:**

- Track regressions found in nightly but missed in PRs
- If >5% slip through, consider tightening strategy

---

## FAQ

### Q: What if a regression slips through?

**A:** Nightly workflow will catch it and create an issue. You can:

1. Revert the problematic PR
2. Fix forward with a new PR
3. Update baseline if intentional

### Q: Can I force performance tests on a PR without label?

**A:** Yes! Two ways:

1. Add `performance` label to PR
2. Go to Actions tab â†’ Performance Tests â†’ Run workflow â†’ Select your branch

### Q: What about main branch protection?

**A:** Performance tests are NOT required checks. They're:

- Advisory (warn but don't block)
- Opt-in (run when needed)
- Nightly will catch issues anyway

### Q: Should I run tests locally before PR?

**A:** Recommended for performance-critical changes:

```bash
make perf-bench-quick    # Takes 3-5 min
make perf-compare        # Compare vs baseline
```

---

## Summary

**Current Strategy: Label-Based âœ…**

- Runs when PR has `performance` label
- Saves ~75% CI costs vs "every PR"
- Balances cost vs catching regressions
- Nightly workflow ensures baselines stay current

**To run performance tests on your PR:**

1. Add label: `performance`
2. Wait for tests to complete (~15 min)
3. Review results in PR comment

**Why nightly is still needed:**

- Updates baselines automatically
- Catches anything that slipped through
- Runs comprehensive 30s benchmarks
- Maintains performance history

**Best of both worlds:** Fast PRs + Accurate baselines! ðŸŽ¯
